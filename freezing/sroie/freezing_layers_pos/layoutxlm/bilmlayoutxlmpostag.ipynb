{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilmlayoutxlmpostag.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "225130fe7f004bc8bfc373b750212af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc5abbf9a9dc45b18b462d20c72548d4",
              "IPY_MODEL_e55daf3fa0234b65b2edb9245797dc53",
              "IPY_MODEL_de1e133953c2423dac775912d728b4f7"
            ],
            "layout": "IPY_MODEL_1c65031e264b45c4a1cf8a758e2372cd"
          }
        },
        "fc5abbf9a9dc45b18b462d20c72548d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0698aba4d2c94ae081aaa1961ae08e27",
            "placeholder": "​",
            "style": "IPY_MODEL_23d6c649f08d4b1f9a6002b9d82830dd",
            "value": "100%"
          }
        },
        "e55daf3fa0234b65b2edb9245797dc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_008c9c6799584312a363787d5d15c4f4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1791a0ac00f44afa81e2b00e0bf01837",
            "value": 2
          }
        },
        "de1e133953c2423dac775912d728b4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e821a7606929443d86a15d6af6788640",
            "placeholder": "​",
            "style": "IPY_MODEL_88ca0bb2ab1e40ce82a3433e2a16358b",
            "value": " 2/2 [00:00&lt;00:00, 67.41it/s]"
          }
        },
        "1c65031e264b45c4a1cf8a758e2372cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0698aba4d2c94ae081aaa1961ae08e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d6c649f08d4b1f9a6002b9d82830dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "008c9c6799584312a363787d5d15c4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1791a0ac00f44afa81e2b00e0bf01837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e821a7606929443d86a15d6af6788640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ca0bb2ab1e40ce82a3433e2a16358b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6263f3d1b2a04effb5eed96dfe00377b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc99e8eb982a492e8f2b6fb67acc8f91",
              "IPY_MODEL_e8fc07d0edaa4f75800f7bafb4f91cc2",
              "IPY_MODEL_93c37db1d33a43be97a048305483e59a"
            ],
            "layout": "IPY_MODEL_abfdf6eb7ec2462e8150287830ad0d20"
          }
        },
        "cc99e8eb982a492e8f2b6fb67acc8f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7755d021f4a4489484a6a7294aa2b6b8",
            "placeholder": "​",
            "style": "IPY_MODEL_8061452f983c497fa22ab6487a5a1e52",
            "value": "100%"
          }
        },
        "e8fc07d0edaa4f75800f7bafb4f91cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565fb488f4ed4b81b04fe919cd7b32d6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a006bfabc1f74d8d9eef86b4c0992d62",
            "value": 1
          }
        },
        "93c37db1d33a43be97a048305483e59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de97c029867a4a6d861d6bae215fab2d",
            "placeholder": "​",
            "style": "IPY_MODEL_959541921c18409b849433d44d1201d3",
            "value": " 1/1 [02:25&lt;00:00, 145.69s/ba]"
          }
        },
        "abfdf6eb7ec2462e8150287830ad0d20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7755d021f4a4489484a6a7294aa2b6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8061452f983c497fa22ab6487a5a1e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "565fb488f4ed4b81b04fe919cd7b32d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a006bfabc1f74d8d9eef86b4c0992d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de97c029867a4a6d861d6bae215fab2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "959541921c18409b849433d44d1201d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43374ec66cdc422b89deb777a2b36ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2dfa27b05744350a34cb6841fc2bcad",
              "IPY_MODEL_14b8b52567314e0f9f7b4f27b4ae15b2",
              "IPY_MODEL_6d8e9b5b012a41ecb9f9f4f2ed1ee32f"
            ],
            "layout": "IPY_MODEL_7564963d826e467a8f52d21cfe6aa2c7"
          }
        },
        "b2dfa27b05744350a34cb6841fc2bcad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761b69834fbd4b17af62ed373bf3e089",
            "placeholder": "​",
            "style": "IPY_MODEL_cc06594c3fb149239a29876a13b149de",
            "value": "100%"
          }
        },
        "14b8b52567314e0f9f7b4f27b4ae15b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11f97aae1ec84a02826a82c6b90acac0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40812ac4c654466282617326c4708f1f",
            "value": 1
          }
        },
        "6d8e9b5b012a41ecb9f9f4f2ed1ee32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae3e7e6013ea48a882800972f62d8c97",
            "placeholder": "​",
            "style": "IPY_MODEL_8239f14f9468492d9562ccfa39537ffe",
            "value": " 1/1 [01:23&lt;00:00, 83.69s/ba]"
          }
        },
        "7564963d826e467a8f52d21cfe6aa2c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "761b69834fbd4b17af62ed373bf3e089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc06594c3fb149239a29876a13b149de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11f97aae1ec84a02826a82c6b90acac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40812ac4c654466282617326c4708f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae3e7e6013ea48a882800972f62d8c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8239f14f9468492d9562ccfa39537ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHVwW9Vv6hPq",
        "outputId": "28634d33-30b5-4246-dc95-fd7ac56b475e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 80.5 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets==2.0.0"
      ],
      "metadata": {
        "id": "8NqrLJ-66iSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdc3ed5-962f-48b5-bfb5-7c554af9d91a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 325 kB 15.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 85.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 76.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 70.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 88.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seqeval\n"
      ],
      "metadata": {
        "id": "JGyZYs2Y6kYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1a2bed-c30b-400d-9da1-f373987def98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyyaml==5.1\n",
        "# workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158)\n",
        "!pip install -q torch==1.11.0+cu113 torchvision==0.9.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install detectron2 that matches pytorch 1.8\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "#!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "#!git clone https://github.com/facebookresearch/detectron2.git\n",
        "#!python -m pip install -e detectron2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iAUy12q6mpD",
        "outputId": "4d3a2a65-eefd-4ffb-e1a4-dfb9b79e6814"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 71 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 81 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 102 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 112 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 122 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 143 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 153 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 163 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 174 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 184 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 194 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 204 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 225 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 235 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 245 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 256 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 274 kB 14.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.9.0+cu113 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.0, 0.4.0+cpu, 0.4.0+cu92, 0.4.1, 0.4.1+cpu, 0.4.1+cu100, 0.4.1+cu92, 0.4.2, 0.4.2+cpu, 0.4.2+cu100, 0.4.2+cu92, 0.5.0, 0.5.0+cpu, 0.5.0+cu100, 0.5.0+cu92, 0.6.0, 0.6.0+cpu, 0.6.0+cu101, 0.6.0+cu92, 0.6.1, 0.6.1+cpu, 0.6.1+cu101, 0.6.1+cu92, 0.7.0, 0.7.0+cpu, 0.7.0+cu101, 0.7.0+cu92, 0.8.0, 0.8.1, 0.8.1+cpu, 0.8.1+cu101, 0.8.1+cu110, 0.8.1+cu92, 0.8.2, 0.8.2+cpu, 0.8.2+cu101, 0.8.2+cu110, 0.8.2+cu92, 0.9.0, 0.9.0+cpu, 0.9.0+cu101, 0.9.0+cu111, 0.9.1, 0.9.1+cpu, 0.9.1+cu101, 0.9.1+cu102, 0.9.1+cu111, 0.10.0, 0.10.0+cpu, 0.10.0+cu102, 0.10.0+cu111, 0.10.0+rocm4.1, 0.10.0+rocm4.2, 0.10.1, 0.10.1+cpu, 0.10.1+cu102, 0.10.1+cu111, 0.10.1+rocm4.1, 0.10.1+rocm4.2, 0.11.0, 0.11.0+cpu, 0.11.0+cu102, 0.11.0+cu111, 0.11.0+cu113, 0.11.0+rocm4.1, 0.11.0+rocm4.2, 0.11.1, 0.11.1+cpu, 0.11.1+cu102, 0.11.1+cu111, 0.11.1+cu113, 0.11.1+rocm4.1, 0.11.1+rocm4.2, 0.11.2, 0.11.2+cpu, 0.11.2+cu102, 0.11.2+cu111, 0.11.2+cu113, 0.11.2+rocm4.1, 0.11.2+rocm4.2, 0.11.3, 0.11.3+cpu, 0.11.3+cu102, 0.11.3+cu111, 0.11.3+cu113, 0.11.3+rocm4.1, 0.11.3+rocm4.2, 0.12.0, 0.12.0+cpu, 0.12.0+cu102, 0.12.0+cu113, 0.12.0+cu115, 0.12.0+rocm4.3.1, 0.12.0+rocm4.5.2, 0.13.0, 0.13.0+cpu, 0.13.0+cu102, 0.13.0+cu113, 0.13.0+cu116, 0.13.0+rocm5.0, 0.13.0+rocm5.1.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torchvision==0.9.0+cu113\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-oifzm2mu\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-oifzm2mu\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.64.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.8.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 81.9 MB/s \n",
            "\u001b[?25hCollecting black==22.3.0\n",
            "  Downloading black-22.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 90.9 MB/s \n",
            "\u001b[?25hCollecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 83.1 MB/s \n",
            "\u001b[?25hCollecting fairscale\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 90.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (2.0.1)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 56.2 MB/s \n",
            "\u001b[?25hCollecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (4.1.1)\n",
            "Collecting platformdirs>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black==22.3.0->detectron2==0.6) (4.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Collecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 87.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (21.3)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from fairscale->detectron2==0.6) (1.11.0+cu113)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black==22.3.0->detectron2==0.6) (3.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.46.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm->detectron2==0.6) (0.12.0+cu113)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime, fairscale\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp37-cp37m-linux_x86_64.whl size=5306494 sha256=559fcc44b747c528cc3426b7a060fb848ad5752ef925403fdf314eca5d59c388\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o322kcn9/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=650fd8a455e6291d3cb19e7d669ac3f2cadf94d5eee0330b88a5dfaf8902519c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=b4107f9eecc4b2c5a4cb0f60e9b866436e43a70ed844bdca64c7cba9222e76e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=32007c94c75be9ad0cbaad1b34081f9073223ff3ff552b6166e4e77e89df8b91\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime fairscale\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, platformdirs, pathspec, omegaconf, mypy-extensions, iopath, click, timm, hydra-core, fvcore, fairscale, black, detectron2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-22.3.0 click-8.1.3 detectron2-0.6 fairscale-0.4.6 fvcore-0.1.5.post20220512 hydra-core-1.2.0 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.2.2 pathspec-0.9.0 platformdirs-2.5.2 portalocker-2.4.0 timm-0.5.4 typed-ast-1.5.4 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YenTV4x5LOoZ",
        "outputId": "0f7ffebc-b308-4d2f-9a1b-54f88b347fd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 15.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##imports\n",
        "import os\n",
        "import glob\n",
        "import json \n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#imports\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import matplotlib\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot, patches\n",
        "from transformers import  LayoutLMv2Processor, LayoutXLMTokenizer, LayoutXLMProcessor,  TrainingArguments, Trainer\n",
        "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
        "from transformers import AutoProcessor,AutoModel, LayoutLMv2PreTrainedModel, LayoutLMv2Model\n",
        "from datasets import load_dataset,Dataset\n",
        "from transformers import get_scheduler\n",
        "from collections import OrderedDict, UserDict\n",
        "from typing import Any, BinaryIO, ContextManager, Dict, List, Optional, Tuple, Union\n",
        "from datasets import load_metric\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from dataclasses import dataclass\n",
        "from dataclasses import fields\n",
        "\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule, AdamW\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n"
      ],
      "metadata": {
        "id": "N0qJGBmv6p1U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "-WLw9nOU8l_O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "\n",
        "\n",
        "datasets = load_dataset(\"darentang/sroie\",cache_dir=None)\n",
        "#datasets = load_dataset(\"nielsr/funsd\",cache_dir=None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "225130fe7f004bc8bfc373b750212af5",
            "fc5abbf9a9dc45b18b462d20c72548d4",
            "e55daf3fa0234b65b2edb9245797dc53",
            "de1e133953c2423dac775912d728b4f7",
            "1c65031e264b45c4a1cf8a758e2372cd",
            "0698aba4d2c94ae081aaa1961ae08e27",
            "23d6c649f08d4b1f9a6002b9d82830dd",
            "008c9c6799584312a363787d5d15c4f4",
            "1791a0ac00f44afa81e2b00e0bf01837",
            "e821a7606929443d86a15d6af6788640",
            "88ca0bb2ab1e40ce82a3433e2a16358b"
          ]
        },
        "id": "NFMg7rPS81nD",
        "outputId": "c825a6dc-3251-418d-d65d-181e97d53f1b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset sroie (/root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "225130fe7f004bc8bfc373b750212af5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#labels\n",
        "labels = datasets['train'].features['ner_tags'].feature.names\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFIIBnvC8uKu",
        "outputId": "475bae7d-969f-4a04-a033-a068abe0d304"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-COMPANY', 'I-COMPANY', 'B-DATE', 'I-DATE', 'B-ADDRESS', 'I-ADDRESS', 'B-TOTAL', 'I-TOTAL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#labels\n",
        "id2label = {v: k for v, k in enumerate(labels)}\n",
        "label2id = {k: v for v, k in enumerate(labels)}\n",
        "label2id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mx8PY7l85Sh",
        "outputId": "70103b98-2224-41dd-e2e8-21ee81c2d68e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-ADDRESS': 5,\n",
              " 'B-COMPANY': 1,\n",
              " 'B-DATE': 3,\n",
              " 'B-TOTAL': 7,\n",
              " 'I-ADDRESS': 6,\n",
              " 'I-COMPANY': 2,\n",
              " 'I-DATE': 4,\n",
              " 'I-TOTAL': 8,\n",
              " 'O': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos2id= {'ADJ': 3,\n",
        " 'ADP': 13,\n",
        " 'ADV': 8,\n",
        " 'AUX': 12,\n",
        " 'CCONJ': 5,\n",
        " 'DET': 7,\n",
        " 'INTJ': 6,\n",
        " 'NOUN': 15,\n",
        " 'NUM': 0,\n",
        " 'PART': 2,\n",
        " 'PRON': 9,\n",
        " 'PROPN': 1,\n",
        " 'PUNCT': 14,\n",
        " 'SYM': 10,\n",
        " 'VERB': 4,\n",
        " 'X': 11,\n",
        " 'pad': 19,\n",
        " 'CONJ':16,\n",
        " 'SCONJ':17,\n",
        " 'SPACE':18\n",
        "\n",
        " }"
      ],
      "metadata": {
        "id": "HZDgz0XT86hN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## preprocess the dataset\n",
        "\n",
        "from transformers import LayoutLMv2FeatureExtractor, LayoutLMv2Tokenizer, LayoutLMv2Processor, AutoTokenizer\n",
        "feature_extractor = LayoutLMv2FeatureExtractor(apply_ocr=False)\n",
        "tokenizer = LayoutXLMTokenizer.from_pretrained('microsoft/layoutxlm-base')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('microsoft/layoutxlm-base')\n",
        "\n",
        "processor = LayoutXLMProcessor(feature_extractor, tokenizer)\n",
        "\n",
        "def read_image_from_dataset(img_data_str: str): \n",
        "    img_data_str_enc = img_data_str.encode(\"utf-8\") \n",
        "    img_data_bytes = base64.b64decode(img_data_str_enc) \n",
        "    img = Image.open(io.BytesIO(img_data_bytes)).convert(\"RGB\") \n",
        "    return np.array(img)\n",
        "# we need to define custom features\n",
        "features = Features({\n",
        "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
        "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
        "    'attention_mask': Sequence(Value(dtype='int64')),\n",
        "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
        "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
        "    'labels': Sequence(ClassLabel(names=labels)),\n",
        "    \"pos_tags\":  Sequence(feature=Value(dtype='int64')),\n",
        "    \n",
        "})\n",
        "# \"pos_tags\":  Sequence(feature=Value(dtype='int64')),\n",
        "\n",
        "def get_tags(input_ids_batch):\n",
        "  ## loop for each dcoment\n",
        "  tags_examples = []\n",
        "  for input_ids in input_ids_batch:\n",
        "\n",
        "    y= processor.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    words=[]\n",
        "    for x in y:\n",
        "      if x.startswith(\"##\"):\n",
        "        words[-1] = words[-1]+x.replace(\"##\", \"\").strip()\n",
        "      else:\n",
        "        words.append(x)\n",
        "\n",
        "    text = \" \".join(words)\n",
        "    text = text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").replace(\"[PAD]\", \"\").strip()\n",
        "    doc = nlp(text)\n",
        "    pos_tags = []\n",
        "    for xx in doc:\n",
        "      pos_tags.append(xx.pos_)\n",
        "    count = 0\n",
        "    total_pos = []\n",
        "    for each in words:\n",
        "      if each not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "        total_pos.append(pos_tags[count])\n",
        "        count += 1\n",
        "      else:\n",
        "          total_pos.append(\"pad\")\n",
        "    assert len(words) == len(total_pos)\n",
        "\n",
        "    def convert_to_original_length(y, tags):\n",
        "   \n",
        "        count = 0\n",
        "        r_tags = []\n",
        "        for index, token in enumerate(y):\n",
        "            \n",
        "            if token.startswith(\"##\"):\n",
        "                r_tags.append(r_tags[-1])\n",
        "            else:\n",
        "                r_tags.append(pos2id[tags[count]])\n",
        "                count += 1\n",
        "        return r_tags\n",
        "\n",
        "    tags_examples.append(convert_to_original_length(y, total_pos))\n",
        "  return tags_examples\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def preprocess_data(examples):\n",
        "  words = examples['words']\n",
        "  boxes = examples['bboxes']\n",
        "  word_labels = examples['ner_tags']\n",
        "  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
        "\n",
        "  encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\n",
        "                             padding=\"max_length\", truncation=True, max_length=512, return_token_type_ids=True) \n",
        "  \n",
        "  encoded_inputs[\"pos_tags\"] = get_tags(encoded_inputs[\"input_ids\"])\n",
        "  return encoded_inputs\n",
        "  \n",
        "  # return encoded_inputs\n",
        "\n",
        "# train_dataset = new_dataset.map(preprocess_data, batched=False, remove_columns=new_dataset.column_names,\n",
        "#                                       features=features)\n",
        "# test_dataset = new_dataset_test.map(preprocess_data, batched=True, remove_columns=new_dataset_test.column_names,\n",
        "#                                       features=features)\n",
        "\n",
        "# x = preprocess_data(datasets[\"train\"][0:1])\n",
        "train_dataset = datasets[\"train\"].map(preprocess_data, batched=True, remove_columns=datasets[\"train\"].column_names,\n",
        "                                      features=features)\n",
        "test_dataset = datasets[\"test\"].map(preprocess_data, batched=True, remove_columns=datasets[\"test\"].column_names,\n",
        "                                       features=features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6263f3d1b2a04effb5eed96dfe00377b",
            "cc99e8eb982a492e8f2b6fb67acc8f91",
            "e8fc07d0edaa4f75800f7bafb4f91cc2",
            "93c37db1d33a43be97a048305483e59a",
            "abfdf6eb7ec2462e8150287830ad0d20",
            "7755d021f4a4489484a6a7294aa2b6b8",
            "8061452f983c497fa22ab6487a5a1e52",
            "565fb488f4ed4b81b04fe919cd7b32d6",
            "a006bfabc1f74d8d9eef86b4c0992d62",
            "de97c029867a4a6d861d6bae215fab2d",
            "959541921c18409b849433d44d1201d3",
            "43374ec66cdc422b89deb777a2b36ed6",
            "b2dfa27b05744350a34cb6841fc2bcad",
            "14b8b52567314e0f9f7b4f27b4ae15b2",
            "6d8e9b5b012a41ecb9f9f4f2ed1ee32f",
            "7564963d826e467a8f52d21cfe6aa2c7",
            "761b69834fbd4b17af62ed373bf3e089",
            "cc06594c3fb149239a29876a13b149de",
            "11f97aae1ec84a02826a82c6b90acac0",
            "40812ac4c654466282617326c4708f1f",
            "ae3e7e6013ea48a882800972f62d8c97",
            "8239f14f9468492d9562ccfa39537ffe"
          ]
        },
        "id": "y5RzaOcX9LEr",
        "outputId": "9ff1da4b-035d-416c-bcba-e68e9b224cdf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/db844fb5c32918721ed233d2411496a5dad53b43140ce01851ee7ee5d1403528.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
            "loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n",
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.4.13.0.json from cache at /root/.cache/huggingface/transformers/117ac0355978336654f3fa4b759a1cb210af1a39ba055bd79ac67bd4d877264b.127a9be4465a314cc3f81f9658b3da53566102ab49058d4bedb01269049f7d5a\n",
            "Model config LayoutLMv2Config {\n",
            "  \"_name_or_path\": \"microsoft/layoutxlm-base\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"fast_qkv\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": false,\n",
            "  \"has_spatial_attention_bias\": false,\n",
            "  \"has_visual_segment_embedding\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6263f3d1b2a04effb5eed96dfe00377b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43374ec66cdc422b89deb777a2b36ed6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5AfsX184UBxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(type=\"torch\")\n",
        "test_dataset.set_format(type=\"torch\")"
      ],
      "metadata": {
        "id": "xoo6GK2l9Mis"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##creating a dataloader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "eCn9NeX59O-4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict, UserDict\n",
        "from typing import Any, BinaryIO, ContextManager, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "class ModelOutput(OrderedDict):\n",
        "    \"\"\"\n",
        "    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n",
        "    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n",
        "    python dictionary.\n",
        "    <Tip warning={true}>\n",
        "    You can't unpack a `ModelOutput` directly. Use the [`~file_utils.ModelOutput.to_tuple`] method to convert it to a\n",
        "    tuple before.\n",
        "    </Tip>\n",
        "    \"\"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        class_fields = fields(self)\n",
        "\n",
        "        # Safety and consistency checks\n",
        "        if not len(class_fields):\n",
        "            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n",
        "        if not all(field.default is None for field in class_fields[1:]):\n",
        "            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n",
        "\n",
        "        first_field = getattr(self, class_fields[0].name)\n",
        "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
        "\n",
        "        if other_fields_are_none and not is_tensor(first_field):\n",
        "            if isinstance(first_field, dict):\n",
        "                iterator = first_field.items()\n",
        "                first_field_iterator = True\n",
        "            else:\n",
        "                try:\n",
        "                    iterator = iter(first_field)\n",
        "                    first_field_iterator = True\n",
        "                except TypeError:\n",
        "                    first_field_iterator = False\n",
        "\n",
        "            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n",
        "            # set the associated fields\n",
        "            if first_field_iterator:\n",
        "                for element in iterator:\n",
        "                    if (\n",
        "                        not isinstance(element, (list, tuple))\n",
        "                        or not len(element) == 2\n",
        "                        or not isinstance(element[0], str)\n",
        "                    ):\n",
        "                        break\n",
        "                    setattr(self, element[0], element[1])\n",
        "                    if element[1] is not None:\n",
        "                        self[element[0]] = element[1]\n",
        "            elif first_field is not None:\n",
        "                self[class_fields[0].name] = first_field\n",
        "        else:\n",
        "            for field in class_fields:\n",
        "                v = getattr(self, field.name)\n",
        "                if v is not None:\n",
        "                    self[field.name] = v\n",
        "\n",
        "    def __delitem__(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def setdefault(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def pop(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        if isinstance(k, str):\n",
        "            inner_dict = {k: v for (k, v) in self.items()}\n",
        "            return inner_dict[k]\n",
        "        else:\n",
        "            return self.to_tuple()[k]\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name in self.keys() and value is not None:\n",
        "            # Don't call self.__setitem__ to avoid recursion errors\n",
        "            super().__setitem__(name, value)\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        # Will raise a KeyException if needed\n",
        "        super().__setitem__(key, value)\n",
        "        # Don't call self.__setattr__ to avoid recursion errors\n",
        "        super().__setattr__(key, value)\n",
        "\n",
        "    def to_tuple(self) -> Tuple[Any]:\n",
        "        \"\"\"\n",
        "        Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
        "        \"\"\"\n",
        "        return tuple(self[k] for k in self.keys())\n",
        "\n"
      ],
      "metadata": {
        "id": "e_MB9QU_9QS6"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pos2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCo-LfytUygc",
        "outputId": "3aabef8f-03c4-4d76-b0e3-4f614df5d9f3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class TokenClassifierOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for outputs of token classification models.\n",
        "    Args:\n",
        "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n",
        "            Classification loss.\n",
        "        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n",
        "            Classification scores (before SoftMax).\n",
        "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
        "            shape `(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
        "            sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LayoutLMv2ForTokenClassification2(LayoutLMv2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.layoutlmv3 = LayoutLMv2Model.from_pretrained(\"microsoft/layoutxlm-base\", output_hidden_states=True)  \n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.embedding= nn.Embedding(21, 768, max_norm=True)\n",
        "        ## for without lstm\n",
        "        #self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        #self.lstm1 =  nn.LSTM(input_size=768, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        #self.lstm2 =  nn.LSTM(input_size=256*2, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.lstm3 =  nn.LSTM(input_size=768, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.lstm4 =  nn.LSTM(input_size=128*2, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Linear(128*2, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        #self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.layoutlmv2.embeddings.word_embeddings\n",
        "\n",
        "   \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        pos_tags: Optional[torch.LongTensor] = None,\n",
        "        bbox: Optional[torch.LongTensor] = None,\n",
        "        image: Optional[torch.FloatTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
        "        Returns:\n",
        "        Examples:\n",
        "        ```python\n",
        "        >>> from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification\n",
        "        >>> from PIL import Image\n",
        "        >>> processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
        "        >>> model = LayoutLMv2ForTokenClassification.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
        "        >>> image = Image.open(\"name_of_your_document - can be a png file, pdf, etc.\").convert(\"RGB\")\n",
        "        >>> words = [\"hello\", \"world\"]\n",
        "        >>> boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\n",
        "        >>> word_labels = [0, 1]\n",
        "        >>> encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\")\n",
        "        >>> outputs = model(**encoding)\n",
        "        >>> loss = outputs.loss\n",
        "        >>> logits = outputs.logits\n",
        "        ```\"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        pos_output = self.embedding(pos_tags)\n",
        "        # print(pos_output.shape)\n",
        "        outputs = self.layoutlmv3(\n",
        "            input_ids=input_ids,\n",
        "            bbox=bbox,\n",
        "            image=image,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "       \n",
        "        sequence_output = torch.stack(list(outputs[2]), dim=0)\n",
        "        # only take the text part of the output representations\n",
        "        \n",
        "        sequence_output=torch.mean(sequence_output,0)[:, :seq_length]\n",
        "        \n",
        "        sequence_output = torch.mean(torch.stack([sequence_output,pos_output], dim=0),0)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        #print(sequence_output.shape)\n",
        "        #sequence_output, _ = self.lstm1(sequence_output)\n",
        "        #sequence_output = self.dropout(sequence_output)\n",
        "        #print(sequence_output.shape)\n",
        "        #sequence_output, _ = self.lstm2(sequence_output)\n",
        "        #print(sequence_output.shape)\n",
        "        sequence_output, _ = self.lstm3(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output, _ = self.lstm4(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output, _ = self.lstm4(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "vNDx9leC9S4O"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
      ],
      "metadata": {
        "id": "PNmu9dSaHo4i"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_metric\n",
        "import json\n",
        "from transformers import get_scheduler\n",
        "\n",
        "model =LayoutLMv2Model.from_pretrained(\"microsoft/layoutxlm-base\", output_hidden_states=True)  \n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "model.config.num_labels = len(id2label)\n",
        "models = LayoutLMv2ForTokenClassification2(model.config)\n",
        "for p in models.layoutlmv3.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "# Set id2label and label2id \n",
        "\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric(\"seqeval\")\n",
        "return_entity_level_metrics = True\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return json.dump({\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "class FunsdTrainer(Trainer):\n",
        "    def get_train_dataloader(self):\n",
        "      return train_dataloader\n",
        "\n",
        "    def get_test_dataloader(self, test_dataset):\n",
        "      return test_dataloader\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule, AdamW\n",
        "#optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "optimizer = AdamW(params=models.parameters(), lr=0.006)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=2000,\n",
        ")\n",
        "args = TrainingArguments(\n",
        "    \n",
        "    output_dir=\"layoutlmv2-finetuned-funsd-v2\", # name of directory to store the checkpoints\n",
        "    max_steps=2000, # we train for a maximum of 1,000 batches\n",
        "    warmup_ratio=0.4, # we warmup a bit\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = FunsdTrainer(\n",
        "    model=models,\n",
        "    args=args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, lr_scheduler)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CK8o6hC9TWe",
        "outputId": "72a15ceb-b0f9-4d25-99a0-fb642cf1576d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n",
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.4.13.0.json from cache at /root/.cache/huggingface/transformers/117ac0355978336654f3fa4b759a1cb210af1a39ba055bd79ac67bd4d877264b.127a9be4465a314cc3f81f9658b3da53566102ab49058d4bedb01269049f7d5a\n",
            "Model config LayoutLMv2Config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"fast_qkv\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": false,\n",
            "  \"has_spatial_attention_bias\": false,\n",
            "  \"has_visual_segment_embedding\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8680422ada73a219d10ded26623c015f44a909e815304488fd43ed77efe03e27.89dd075ca1cb2d01705599c66b2867ecedda5e15879080b08735d2dbdf3631b7\n",
            "using `AvgPool2d` instead of `AdaptiveAvgPool2d`\n",
            "Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2Model: ['layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked']\n",
            "- This IS expected if you are initializing LayoutLMv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LayoutLMv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of LayoutLMv2Model were initialized from the model checkpoint at microsoft/layoutxlm-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMv2Model for predictions without further training.\n",
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f2d84dd26136732523aedb80cf0fe429d11a0dc06d8068706be3f62bdfc085f7.ef0f8c632b10ee11ca58d7ca82bf36d0873dec5950e09ee245d7f621d93fecb0\n",
            "loading configuration file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/config.4.13.0.json from cache at /root/.cache/huggingface/transformers/117ac0355978336654f3fa4b759a1cb210af1a39ba055bd79ac67bd4d877264b.127a9be4465a314cc3f81f9658b3da53566102ab49058d4bedb01269049f7d5a\n",
            "Model config LayoutLMv2Config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"eos_token_id\": 2,\n",
            "  \"fast_qkv\": false,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": false,\n",
            "  \"has_spatial_attention_bias\": false,\n",
            "  \"has_visual_segment_embedding\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"tokenizer_class\": \"LayoutXLMTokenizer\",\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/layoutxlm-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8680422ada73a219d10ded26623c015f44a909e815304488fd43ed77efe03e27.89dd075ca1cb2d01705599c66b2867ecedda5e15879080b08735d2dbdf3631b7\n",
            "using `AvgPool2d` instead of `AdaptiveAvgPool2d`\n",
            "Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2Model: ['layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked']\n",
            "- This IS expected if you are initializing LayoutLMv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LayoutLMv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of LayoutLMv2Model were initialized from the model checkpoint at microsoft/layoutxlm-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMv2Model for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using cuda_amp half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "# predictions, labels, metrics = trainer.predict(test_dataset)\n"
      ],
      "metadata": {
        "id": "EifKkBYB9YrF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "f9e19d8a-0f52-422c-fa2a-99e04b85a446"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 626\n",
            "  Num Epochs = 26\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 11:40, Epoch 25/26]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.108500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.008800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.004100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-500\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-500/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-1000\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1000/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-1500\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1500/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-2000\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-2000/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-2000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=0.03611990427970886, metrics={'train_runtime': 700.9092, 'train_samples_per_second': 22.827, 'train_steps_per_second': 2.853, 'total_flos': 8620642620979200.0, 'train_loss': 0.03611990427970886, 'epoch': 25.32})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")"
      ],
      "metadata": {
        "id": "C3127xG8qR6-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "eval_loss = 0.0\n",
        "nb_eval_steps = 0\n",
        "preds = None\n",
        "out_label_ids = None\n",
        "\n",
        "# put model in evaluation mode\n",
        "models.eval()\n",
        "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        print(batch.keys())\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        bbox = batch[\"bbox\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        image = batch['image'].to(device)\n",
        "        pos_tags=batch[\"pos_tags\"].to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = models(pos_tags=pos_tags,image = image , input_ids=input_ids, bbox=bbox, attention_mask=attention_mask,labels=labels)\n",
        "        # get the loss and logits\n",
        "        tmp_eval_loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        eval_loss += tmp_eval_loss.item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # compute the predictions\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(\n",
        "                out_label_ids, labels.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "\n",
        "# compute average evaluation loss\n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "preds = np.argmax(preds, axis=2)\n",
        "\n",
        "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "\n",
        "for i in range(out_label_ids.shape[0]):\n",
        "    for j in range(out_label_ids.shape[1]):\n",
        "        if out_label_ids[i, j] != -100:\n",
        "            out_label_list[i].append(id2label[out_label_ids[i][j]])\n",
        "            preds_list[i].append(id2label[preds[i][j]])\n",
        "\n",
        "results = {\n",
        "    \"loss\": eval_loss,\n",
        "    \"precision\": precision_score(out_label_list, preds_list),\n",
        "    \"recall\": recall_score(out_label_list, preds_list),\n",
        "    \"f1\": f1_score(out_label_list, preds_list),\n",
        "}\n",
        "print(results)"
      ],
      "metadata": {
        "id": "it8lQd2QGgcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77db0f2a-a40d-4807-886b-ee1ad6f4d769"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/44 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   2%|▏         | 1/44 [00:00<00:17,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   5%|▍         | 2/44 [00:00<00:14,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   7%|▋         | 3/44 [00:00<00:13,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   9%|▉         | 4/44 [00:01<00:12,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  11%|█▏        | 5/44 [00:01<00:11,  3.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  14%|█▎        | 6/44 [00:01<00:11,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  16%|█▌        | 7/44 [00:02<00:11,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  18%|█▊        | 8/44 [00:02<00:10,  3.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|██        | 9/44 [00:02<00:10,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  23%|██▎       | 10/44 [00:03<00:10,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  25%|██▌       | 11/44 [00:03<00:09,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  27%|██▋       | 12/44 [00:03<00:09,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  30%|██▉       | 13/44 [00:03<00:09,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  32%|███▏      | 14/44 [00:04<00:08,  3.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  34%|███▍      | 15/44 [00:04<00:08,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  36%|███▋      | 16/44 [00:04<00:08,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  39%|███▊      | 17/44 [00:05<00:07,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  41%|████      | 18/44 [00:05<00:07,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  43%|████▎     | 19/44 [00:05<00:07,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  45%|████▌     | 20/44 [00:05<00:07,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  48%|████▊     | 21/44 [00:06<00:06,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  50%|█████     | 22/44 [00:06<00:06,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  52%|█████▏    | 23/44 [00:06<00:06,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  55%|█████▍    | 24/44 [00:07<00:05,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  57%|█████▋    | 25/44 [00:07<00:05,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  59%|█████▉    | 26/44 [00:07<00:05,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  61%|██████▏   | 27/44 [00:08<00:05,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  64%|██████▎   | 28/44 [00:08<00:04,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  66%|██████▌   | 29/44 [00:08<00:04,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  68%|██████▊   | 30/44 [00:08<00:04,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  70%|███████   | 31/44 [00:09<00:03,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  73%|███████▎  | 32/44 [00:09<00:03,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  75%|███████▌  | 33/44 [00:09<00:03,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  77%|███████▋  | 34/44 [00:10<00:02,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  80%|███████▉  | 35/44 [00:10<00:02,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  82%|████████▏ | 36/44 [00:10<00:02,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  84%|████████▍ | 37/44 [00:11<00:02,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  86%|████████▋ | 38/44 [00:11<00:01,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  89%|████████▊ | 39/44 [00:11<00:01,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  91%|█████████ | 40/44 [00:11<00:01,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  93%|█████████▎| 41/44 [00:12<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  95%|█████████▌| 42/44 [00:12<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  98%|█████████▊| 43/44 [00:12<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 44/44 [00:13<00:00,  3.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.03718666518761893, 'precision': 0.9066286528866714, 'recall': 0.9164265129682997, 'f1': 0.9115012540308133}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NYJ0orSFqPM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}