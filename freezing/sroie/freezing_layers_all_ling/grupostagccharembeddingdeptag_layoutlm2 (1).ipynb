{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHVwW9Vv6hPq",
        "outputId": "17785ce8-23a0-45ff-bf09-6ff4fb5fa106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NqrLJ-66iSw"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGyZYs2Y6kYQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iAUy12q6mpD",
        "outputId": "d17117c0-76a8-49bb-a50d-5ed4540a6654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 1636958208 bytes == 0x2388000 @  0x7f3f424ae1e7 0x4a3940 0x5b438c 0x5ea94f 0x5939cb 0x594cd3 0x5d0ecb 0x59aeca 0x515655 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549e0e 0x4bca8a 0x59c019\n",
            "tcmalloc: large alloc 1636958208 bytes == 0x63ca8000 @  0x7f3f424ae1e7 0x4a3940 0x5b438c 0x64cfe7 0x59b076 0x515655 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576\n",
            "tcmalloc: large alloc 1636958208 bytes == 0xc55c8000 @  0x7f3f424ae1e7 0x4a3940 0x59b5e2 0x63a515 0x63bd66 0x63be16 0x59afff 0x515655 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x593dd7 0x511e2c 0x593dd7 0x511e2c 0x549e0e 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576\n",
            "tcmalloc: large alloc 1636958208 bytes == 0x126ee8000 @  0x7f3f424ae1e7 0x4a3940 0x59b6f0 0x59f499 0x4d3969 0x512147 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x549e0e 0x593fce 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x593dd7 0x5118f8 0x593dd7 0x511e2c 0x593dd7 0x511e2c\n",
            "tcmalloc: large alloc 2046197760 bytes == 0x63ca8000 @  0x7f3f424af615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549e0e 0x593fce\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.9.0+cu113 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.0, 0.4.0+cpu, 0.4.0+cu92, 0.4.1, 0.4.1+cpu, 0.4.1+cu100, 0.4.1+cu92, 0.4.2, 0.4.2+cpu, 0.4.2+cu100, 0.4.2+cu92, 0.5.0, 0.5.0+cpu, 0.5.0+cu100, 0.5.0+cu92, 0.6.0, 0.6.0+cpu, 0.6.0+cu101, 0.6.0+cu92, 0.6.1, 0.6.1+cpu, 0.6.1+cu101, 0.6.1+cu92, 0.7.0, 0.7.0+cpu, 0.7.0+cu101, 0.7.0+cu92, 0.8.0, 0.8.1, 0.8.1+cpu, 0.8.1+cu101, 0.8.1+cu110, 0.8.1+cu92, 0.8.2, 0.8.2+cpu, 0.8.2+cu101, 0.8.2+cu110, 0.8.2+cu92, 0.9.0, 0.9.0+cpu, 0.9.0+cu101, 0.9.0+cu111, 0.9.1, 0.9.1+cpu, 0.9.1+cu101, 0.9.1+cu102, 0.9.1+cu111, 0.10.0, 0.10.0+cpu, 0.10.0+cu102, 0.10.0+cu111, 0.10.0+rocm4.1, 0.10.0+rocm4.2, 0.10.1, 0.10.1+cpu, 0.10.1+cu102, 0.10.1+cu111, 0.10.1+rocm4.1, 0.10.1+rocm4.2, 0.11.0, 0.11.0+cpu, 0.11.0+cu102, 0.11.0+cu111, 0.11.0+cu113, 0.11.0+rocm4.1, 0.11.0+rocm4.2, 0.11.1, 0.11.1+cpu, 0.11.1+cu102, 0.11.1+cu111, 0.11.1+cu113, 0.11.1+rocm4.1, 0.11.1+rocm4.2, 0.11.2, 0.11.2+cpu, 0.11.2+cu102, 0.11.2+cu111, 0.11.2+cu113, 0.11.2+rocm4.1, 0.11.2+rocm4.2, 0.11.3, 0.11.3+cpu, 0.11.3+cu102, 0.11.3+cu111, 0.11.3+cu113, 0.11.3+rocm4.1, 0.11.3+rocm4.2, 0.12.0, 0.12.0+cpu, 0.12.0+cu102, 0.12.0+cu113, 0.12.0+cu115, 0.12.0+rocm4.3.1, 0.12.0+rocm4.5.2, 0.13.0, 0.13.0+cpu, 0.13.0+cu102, 0.13.0+cu113, 0.13.0+cu116, 0.13.0+rocm5.0, 0.13.0+rocm5.1.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torchvision==0.9.0+cu113\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-9wqszm6x\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-9wqszm6x\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.64.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.8.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.5.post20220512)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.2.2)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: black==22.3.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (22.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.6.7)\n",
            "Requirement already satisfied: fairscale in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (21.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (8.1.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (0.4.3)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (1.5.4)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (2.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black==22.3.0->detectron2==0.6) (4.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.9.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.5.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from fairscale->detectron2==0.6) (1.12.0+cu113)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black==22.3.0->detectron2==0.6) (3.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.47.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm->detectron2==0.6) (0.13.0+cu113)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyyaml==5.1\n",
        "# workaround: install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9 (issue: https://github.com/facebookresearch/detectron2/issues/3158)\n",
        "!pip install -q torch==1.11.0+cu113 torchvision==0.9.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install detectron2 that matches pytorch 1.8\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "#!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "#!git clone https://github.com/facebookresearch/detectron2.git\n",
        "#!python -m pip install -e detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0qJGBmv6p1U"
      },
      "outputs": [],
      "source": [
        "##imports\n",
        "import os\n",
        "import glob\n",
        "import json \n",
        "import random\n",
        "from pathlib import Path\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" \n",
        "import torch \n",
        "import numpy as np\n",
        "import random\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display\n",
        "import matplotlib\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot, patches\n",
        "from transformers import LayoutLMv2Processor\n",
        "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
        "from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WLw9nOU8l_O"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import TrainingArguments, Trainer,LayoutLMv2Model, LayoutLMv2PreTrainedModel\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "from dataclasses import dataclass\n",
        "from dataclasses import fields\n",
        "from datasets import load_dataset\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huJ0QRiEwWEc"
      },
      "outputs": [],
      "source": [
        "!rm -r /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QHbrCxFNkvwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zkvrZ8z5kv2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFMg7rPS81nD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "246aec5c31324290a68ef4471b7d75c3",
            "276eee742f474c9d94a341d38a6ebaf8",
            "4561803ed1294a08a68d1fb9223d71b6",
            "a4fd1fa070094f089d5c58a38bdd16d8",
            "aa02c64aa2fe43329861349fd2e8a716",
            "2419b791e9044f17a8b4674ada744794",
            "bb08562dcb3b4ba79edffdf5d0312e5e",
            "c1d61a7c618640c99dfcc6e4c9ac697d",
            "90ffb128171c4ba69cbcfce042021d36",
            "828f1097a4194979926635b9c0d499ac",
            "de0d538f088344d482619681231208fe",
            "64297f9091674e51a1fa974076162394",
            "be1770715bbb4aaea456374fd98dc49a",
            "99f038d419334c2784c6790c5b26d9a6",
            "0a92346fa836489690824fb5967105b3",
            "422f86fe0b1f47e7a3d3e9665a78f1ca",
            "c0f8b1dbd45f47f3b958c6f6599d6547",
            "0d96c3d182f1490f9b2b68b90e2e5fba",
            "56faee18ba0747c6a50099238e64575b",
            "cb796ed667134387bb75ba53a303364c",
            "ea279761b3904828b1193ac8bac88cb7",
            "010fc26ad69543628aaa6124f857f2cd",
            "cddd4190bdf64465a620e828f3a33a39",
            "b4bfcb3ddbc44daa85afc9d2a75b6822",
            "14da34fff6c24d40a253abe9723d6deb",
            "d9055168444c409b930bac9a6138caac",
            "f8e85cae3fbd44c6af1487b3c2edd36c",
            "d4669cfc1bb7491fb875c09cb890670b",
            "01c7951c37294ae5873ca753b1de88f9",
            "a4fbe9d0466c4004ad65c84f60ee5d5e",
            "6fa0399073b547ed8018ce4850bc59ab",
            "9aceb33b3a244f38b8b31bac9efd3fc1",
            "1ac110272a7d4520a4f2f7101d0963c6",
            "cb892fa67f374b1285d808d93c2903d9",
            "97de47bad8c443649bae9cebbbfc2666",
            "89f59220bb5842e39b8d561378928d3a",
            "7d33d6638b9644a9b89cfeac03280146",
            "5ef2631b6615438f8f6c2e609eca43f7",
            "cb406b03d0dc440ca99a205cfa33df62",
            "24388f62ab9a4fc5bd7a23317802cfd8",
            "d326a579c668430f987d0d341776b752",
            "9789dc9a7dcd4c5ba36abbe2579dee9b",
            "fa26524f49f04150b688e38e597cd0bd",
            "4eddc6e776684052941d07d8b7e36b01",
            "59b792acc3114c86bfbdd4af0be07232",
            "2eaf1ab09c4b48fba5734667a48b5073",
            "7ccc4a870dd0424a89f8124a7673372d",
            "2f3e05f28dde40589ccbf083139ff001",
            "cbb44d65cddb4a0c89eb5a3db6367632",
            "22b4caeafc65455fb5fc5f9e5750e2bf",
            "7d4dd5a92bd34149818fc12c7acf38ca",
            "5814ff9c3e68418d9e5a71713136f10e",
            "6078fc2d150e423293d2d226fc4cc92b",
            "a9b79b07c3654f6e95e7442d923641fa",
            "291bf0c8c5fd461f9715a6cad21cc4a3"
          ]
        },
        "outputId": "23241bfa-73eb-4a09-d78d-c004c29f3242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset sroie/sroie to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "246aec5c31324290a68ef4471b7d75c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64297f9091674e51a1fa974076162394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cddd4190bdf64465a620e828f3a33a39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb892fa67f374b1285d808d93c2903d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sroie downloaded and prepared to /root/.cache/huggingface/datasets/darentang___sroie/sroie/1.0.0/26ed9374c9a15a1d2f44fd8886f679076e1a1fd7da2d53726d6e58a99436c506. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b792acc3114c86bfbdd4af0be07232"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# data loading\n",
        "\n",
        "\n",
        "datasets = load_dataset(\"darentang/sroie\",cache_dir=None)\n",
        "#datasets = load_dataset(\"nielsr/funsd\",cache_dir=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFIIBnvC8uKu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c21195a-81e0-4f21-990d-730968a3d5c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-COMPANY', 'I-COMPANY', 'B-DATE', 'I-DATE', 'B-ADDRESS', 'I-ADDRESS', 'B-TOTAL', 'I-TOTAL']\n"
          ]
        }
      ],
      "source": [
        "#labels\n",
        "labels = datasets['train'].features['ner_tags'].feature.names\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mx8PY7l85Sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7730d93-f5d6-46ab-ce65-da0b4b89829a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-ADDRESS': 5,\n",
              " 'B-COMPANY': 1,\n",
              " 'B-DATE': 3,\n",
              " 'B-TOTAL': 7,\n",
              " 'I-ADDRESS': 6,\n",
              " 'I-COMPANY': 2,\n",
              " 'I-DATE': 4,\n",
              " 'I-TOTAL': 8,\n",
              " 'O': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "#labels\n",
        "id2label = {v: k for v, k in enumerate(labels)}\n",
        "label2id = {k: v for v, k in enumerate(labels)}\n",
        "label2id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZDgz0XT86hN"
      },
      "outputs": [],
      "source": [
        "pos2id= {'ADJ': 3,\n",
        " 'ADP': 13,\n",
        " 'ADV': 8,\n",
        " 'AUX': 12,\n",
        " 'CCONJ': 5,\n",
        " 'DET': 7,\n",
        " 'INTJ': 6,\n",
        " 'NOUN': 15,\n",
        " 'NUM': 0,\n",
        " 'PART': 2,\n",
        " 'PRON': 9,\n",
        " 'PROPN': 1,\n",
        " 'PUNCT': 14,\n",
        " 'SYM': 10,\n",
        " 'VERB': 4,\n",
        " 'X': 11,\n",
        " 'PAD': 19,\n",
        " 'CONJ':16,\n",
        " 'SCONJ':17,\n",
        " 'SPACE':18\n",
        "\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRoCo-ST91MK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a08c1d-469f-40bd-8672-d212bdcd81de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image_path'],\n",
              "        num_rows: 626\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image_path'],\n",
              "        num_rows: 347\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgMIi7Cw94nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0eb4d2-10ce-4dec-eb21-5217ec10c1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for x in \"Life is like a box of chocolates.\":\n",
        "  if x is \" \":\n",
        "    count  += 1\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dep = [\"ACL\", \"ACOMP\", \"ADVCL\", \"ADVMOD\", \"AGENT\", \"AMOD\", \"APPOS\", \"ATTR\", \"AUX\", \"AUXPASS\", \"CASE\", \"CC\", \"CCOMP\", \"COMPOUND\", \"CONJ\", \"CSUBJ\", \"CSUBJPASS\", \"DATIVE\", \"DEP\", \"DET\", \"DOBJ\", \"EXPL\", \"INTJ\", \"MARK\", \"META\", \"NEG\", \"NOUNMOD\", \"NPMOD\", \"NSUBJ\", \"NSUBJPASS\", \"NUMMOD\", \"OPRD\", \"PARATAXIS\", \"PCOMP\", \"POBJ\", \"POSS\", \"PRECONJ\", \"PREDET\", \"PREP\", \"PRT\", \"PUNCT\", \"QUANTMOD\", \"RELCL\", \"ROOT\", \"XCOMP\",\"PAD\", \"COMPLM\", \"INFMOD\", \"PARTMOD\", \"HMOD\", \"HYPH\", \"IOBJ\", \"NUM\", \"NUMBER\", \"NMOD\", \"NN\", \"NPADVMOD\", \"POSSESSIVE\", \"RCMOD\", \"PAD\"]\n",
        "dep2id = {v:k for k,v in enumerate(dep)}"
      ],
      "metadata": {
        "id": "c7gb_X8kRX1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5RzaOcX9LEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "838ee8a405034ce89620b601eee7c7c4",
            "75f5be29e26b4d8da23ae63327f3966f",
            "98b544367f2d45e48fb8d5b772348c74",
            "174327660f484a6688301e98f9537f48",
            "be6f0ddee53b4c928dec0ceacab867be",
            "02bafff4fce3493081c64ae4b8782591",
            "0cbc8581e58542e9b27fbec15075fe15",
            "491f437e2baf4e25afd89caff5dfab69",
            "8a6a1fea07884d1fa913249791ac06a2",
            "eb243e14b3114e0e963bc437042dc29e",
            "f980c393edad4f26a4b696971b7dc627",
            "7634c69e934749149650f0acb5a39f68",
            "47fef0a821fc409196bfb8be3756056a",
            "7932d2cf8cc54b3fb214e28dba750df7",
            "04e0f2d0a1c9402399c593e5834b3a21",
            "153d9521b43a46e88bfb1492bc0a301b",
            "b56389c21ec14cb3a981ff85f0f11704",
            "85ed8edba42241e4967770c8120b69c3",
            "1e8e73cd9f1c44adb3ea1cc88624a058",
            "2d4a049eed6c401ba081983b4c7c93f9",
            "ed99d3853981478e9262c9d92cfa65a4",
            "0ee2b543b20b4e81a2f5ac591a2537ff"
          ]
        },
        "outputId": "c4f5d11b-4e3a-42e7-b3f3-4b5be5b00e88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "838ee8a405034ce89620b601eee7c7c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7634c69e934749149650f0acb5a39f68"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
        "from transformers import AutoTokenizer\n",
        "## preprocess the dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer_char = AutoTokenizer.from_pretrained('google/byt5-small')\n",
        "\n",
        "def char_emb_input_ids( input_ids_batch):\n",
        "  texts= processor.tokenizer.batch_decode(input_ids_batch)\n",
        "  model_input_ = tokenizer_char(texts, padding=True, truncation=True).input_ids\n",
        "  import gc\n",
        "  gc.collect()\n",
        "\n",
        "  return model_input_\n",
        "\n",
        "\n",
        "# we need to define custom features\n",
        "features = Features({\n",
        "    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n",
        "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
        "    'embedding_char_ids': Sequence(feature=Value(dtype='int64')),\n",
        "    'attention_mask': Sequence(Value(dtype='int64')),\n",
        "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
        "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
        "    'labels': Sequence(ClassLabel(names=labels)),\n",
        "    \"pos_tags\":  Sequence(feature=Value(dtype='int64')),\n",
        "    \"dep_tags\":  Sequence(feature=Value(dtype='int64')),\n",
        "    \n",
        "})\n",
        "\n",
        "\n",
        "def get_tags(input_ids_batch, entype=\"pos\"):\n",
        "  ## loop for each dcoment\n",
        "  tags_examples = []\n",
        "  if entype == \"pos\":\n",
        "    tag2id = pos2id\n",
        "  else:\n",
        "    tag2id = dep2id\n",
        "\n",
        "  for input_ids in input_ids_batch:\n",
        "\n",
        "    y= processor.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    words=[]\n",
        "    for x in y:\n",
        "      if x.startswith(\"##\"):\n",
        "        words[-1] = words[-1]+x.replace(\"##\", \"\").strip()\n",
        "      else:\n",
        "        words.append(x)\n",
        "\n",
        "    text = \" \".join(words)\n",
        "    text = text.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").replace(\"[PAD]\", \"\").strip()\n",
        "    doc = nlp(text)\n",
        "    pos_tags = []\n",
        "    for xx in doc:\n",
        "      if entype == \"pos\":\n",
        "        pos = xx.pos_\n",
        "        \n",
        "        pos_tags.append(pos)\n",
        "      else:\n",
        "        dep = xx.dep_.upper()\n",
        "       \n",
        "        pos_tags.append(dep)\n",
        "        \n",
        "        \n",
        "    count = 0\n",
        "    total_pos = []\n",
        "    for each in words:\n",
        "      if each not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "        total_pos.append(pos_tags[count])\n",
        "        count += 1\n",
        "      else:\n",
        "          total_pos.append(\"PAD\")\n",
        "    assert len(words) == len(total_pos)\n",
        "\n",
        "    def convert_to_original_length(y, tags):\n",
        "   \n",
        "        count = 0\n",
        "        r_tags = []\n",
        "        for index, token in enumerate(y):\n",
        "            \n",
        "            if token.startswith(\"##\"):\n",
        "                r_tags.append(r_tags[-1])\n",
        "            else:\n",
        "                r_tags.append(tag2id[tags[count]])\n",
        "                count += 1\n",
        "        return r_tags\n",
        "\n",
        "    tags_examples.append(convert_to_original_length(y, total_pos))\n",
        "  return tags_examples\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def preprocess_data(examples):\n",
        "  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n",
        "  words = examples['words']\n",
        "  boxes = examples['bboxes']\n",
        "  word_labels = examples['ner_tags']\n",
        "  # input_ids_t5 = tokenizer_t5([\" \".join(each) for each in words], padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  encoded_inputs = processor(images, words, boxes=boxes, word_labels=word_labels,\n",
        "                             padding=\"max_length\", truncation=True)\n",
        "  encoded_inputs[\"embedding_char_ids\"] = char_emb_input_ids(encoded_inputs[\"input_ids\"])\n",
        "  encoded_inputs[\"dep_tags\"] = get_tags(encoded_inputs[\"input_ids\"], entype=\"dep\")\n",
        "  encoded_inputs[\"pos_tags\"] = get_tags(encoded_inputs[\"input_ids\"])\n",
        "\n",
        "  return encoded_inputs\n",
        "  \n",
        "train_dataset = datasets[\"train\"].map(preprocess_data,batched=True, remove_columns=datasets[\"train\"].column_names,\n",
        "                                      features=features)\n",
        "test_dataset = datasets[\"test\"].map(preprocess_data, batched=True, remove_columns=datasets[\"test\"].column_names,\n",
        "                                       features=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoo6GK2l9Mis"
      },
      "outputs": [],
      "source": [
        "train_dataset.set_format(type=\"torch\")\n",
        "test_dataset.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCn9NeX59O-4"
      },
      "outputs": [],
      "source": [
        "\n",
        "##creating a dataloader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_MB9QU_9QS6"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict, UserDict\n",
        "from typing import Any, BinaryIO, ContextManager, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "class ModelOutput(OrderedDict):\n",
        "    \"\"\"\n",
        "    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n",
        "    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n",
        "    python dictionary.\n",
        "    <Tip warning={true}>\n",
        "    You can't unpack a `ModelOutput` directly. Use the [`~file_utils.ModelOutput.to_tuple`] method to convert it to a\n",
        "    tuple before.\n",
        "    </Tip>\n",
        "    \"\"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        class_fields = fields(self)\n",
        "\n",
        "        # Safety and consistency checks\n",
        "        if not len(class_fields):\n",
        "            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n",
        "        if not all(field.default is None for field in class_fields[1:]):\n",
        "            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n",
        "\n",
        "        first_field = getattr(self, class_fields[0].name)\n",
        "        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n",
        "\n",
        "        if other_fields_are_none and not is_tensor(first_field):\n",
        "            if isinstance(first_field, dict):\n",
        "                iterator = first_field.items()\n",
        "                first_field_iterator = True\n",
        "            else:\n",
        "                try:\n",
        "                    iterator = iter(first_field)\n",
        "                    first_field_iterator = True\n",
        "                except TypeError:\n",
        "                    first_field_iterator = False\n",
        "\n",
        "            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n",
        "            # set the associated fields\n",
        "            if first_field_iterator:\n",
        "                for element in iterator:\n",
        "                    if (\n",
        "                        not isinstance(element, (list, tuple))\n",
        "                        or not len(element) == 2\n",
        "                        or not isinstance(element[0], str)\n",
        "                    ):\n",
        "                        break\n",
        "                    setattr(self, element[0], element[1])\n",
        "                    if element[1] is not None:\n",
        "                        self[element[0]] = element[1]\n",
        "            elif first_field is not None:\n",
        "                self[class_fields[0].name] = first_field\n",
        "        else:\n",
        "            for field in class_fields:\n",
        "                v = getattr(self, field.name)\n",
        "                if v is not None:\n",
        "                    self[field.name] = v\n",
        "\n",
        "    def __delitem__(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def setdefault(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def pop(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n",
        "\n",
        "    def __getitem__(self, k):\n",
        "        if isinstance(k, str):\n",
        "            inner_dict = {k: v for (k, v) in self.items()}\n",
        "            return inner_dict[k]\n",
        "        else:\n",
        "            return self.to_tuple()[k]\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name in self.keys() and value is not None:\n",
        "            # Don't call self.__setitem__ to avoid recursion errors\n",
        "            super().__setitem__(name, value)\n",
        "        super().__setattr__(name, value)\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        # Will raise a KeyException if needed\n",
        "        super().__setitem__(key, value)\n",
        "        # Don't call self.__setattr__ to avoid recursion errors\n",
        "        super().__setattr__(key, value)\n",
        "\n",
        "    def to_tuple(self) -> Tuple[Any]:\n",
        "        \"\"\"\n",
        "        Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
        "        \"\"\"\n",
        "        return tuple(self[k] for k in self.keys())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeF548CkXYv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645fc66d-22d2-4313-e719-2016af1ac713"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNDx9leC9S4O"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TokenClassifierOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for outputs of token classification models.\n",
        "    Args:\n",
        "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n",
        "            Classification loss.\n",
        "        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n",
        "            Classification scores (before SoftMax).\n",
        "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
        "            shape `(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
        "            sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LayoutLMv2ForTokenClassification2(LayoutLMv2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.layoutlmv2 = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", output_hidden_states=True)  \n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.embedding_dep= nn.Embedding(60, 50, max_norm=True)\n",
        "        self.embedding_pos= nn.Embedding(21, 50, max_norm=True)\n",
        "        self.model_char = T5EncoderModel.from_pretrained('google/byt5-small')\n",
        "        self.model_char.to(device)\n",
        "        \n",
        "        # print(self.charemb.model_char(train_dataset[0][\"embedding_char_ids\"].to(device)))\n",
        "\n",
        "        ## for without lstm\n",
        "        #self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        #self.lstm1 =  nn.LSTM(input_size=768, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        #self.lstm2 =  nn.LSTM(input_size=256*2, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.lstm3 =  nn.GRU(input_size=768+736+50+50, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.lstm4 =  nn.GRU(input_size=128*2, hidden_size=128, batch_first=True, bidirectional=True)\n",
        "        self.classifier = nn.Linear(128*2, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        #self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.layoutlmv2.embeddings.word_embeddings\n",
        "        \n",
        "    def char_emb_to_word(self,model_inputs):\n",
        "      ## loop for each document\n",
        "      total_emb = []\n",
        "      \n",
        "      with torch.no_grad(): \n",
        "          # print(torch.tensor([[2,3]]).shape)\n",
        "          # print(model_inputs.shape)\n",
        "          embeddings_chars = self.model_char(model_inputs)[\"last_hidden_state\"]\n",
        "          # print(model_inputs.shape)\n",
        "          for i, embeddings_char in enumerate(embeddings_chars):\n",
        "            spaces = (model_inputs[i] == 35).nonzero().flatten()\n",
        "            truc = int(embeddings_char.shape[-1]/2)\n",
        "            final = torch.zeros((512, truc))\n",
        "            count = 0\n",
        "            index = 0\n",
        "            for sp in spaces:\n",
        "              final[index,:] = torch.mean(embeddings_char[count:sp, :truc], 0)\n",
        "              count  = sp+1\n",
        "              index += 1\n",
        "            total_emb.append(final)\n",
        "      return torch.stack(total_emb, 0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        \n",
        "        embedding_char_ids: Optional[torch.Tensor] = None,\n",
        "        pos_tags: Optional[torch.LongTensor] = None,\n",
        "        dep_tags: Optional[torch.LongTensor] = None,\n",
        "        bbox: Optional[torch.LongTensor] = None,\n",
        "        image: Optional[torch.FloatTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
        "        Returns:\n",
        "        Examples:\n",
        "        ```python\n",
        "        >>> from transformers import LayoutLMv2Processor, LayoutLMv2ForTokenClassification\n",
        "        >>> from PIL import Image\n",
        "        >>> processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
        "        >>> model = LayoutLMv2ForTokenClassification.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
        "        >>> image = Image.open(\"name_of_your_document - can be a png file, pdf, etc.\").convert(\"RGB\")\n",
        "        >>> words = [\"hello\", \"world\"]\n",
        "        >>> boxes = [[1, 2, 3, 4], [5, 6, 7, 8]]  # make sure to normalize your bounding boxes\n",
        "        >>> word_labels = [0, 1]\n",
        "        >>> encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\")\n",
        "        >>> outputs = model(**encoding)\n",
        "        >>> loss = outputs.loss\n",
        "        >>> logits = outputs.logits\n",
        "        ```\"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        # print(embedding_char_ids.shape)\n",
        "        char_emb =  self.char_emb_to_word(embedding_char_ids.to(device))\n",
        "        \n",
        "        dep_output = self.embedding_dep(dep_tags)\n",
        "        pos_output = self.embedding_pos(pos_tags)\n",
        "        total_emb = torch.cat([dep_output,pos_output],2)\n",
        "      \n",
        "      \n",
        "        # if torch.isnan(char_emb).any():\n",
        "        #   print(embedding_char_ids)\n",
        "        \n",
        "        pos_output = torch.cat((char_emb.to(device), total_emb), 2)\n",
        "        \n",
        "        # print(pos_output.shape)\n",
        "        outputs = self.layoutlmv2(\n",
        "            input_ids=input_ids,\n",
        "            bbox=bbox,\n",
        "            image=image,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "       \n",
        "        sequence_output = torch.stack(list(outputs[2]), dim=0)\n",
        "        # only take the text part of the output representations\n",
        "        \n",
        "        sequence_output=torch.mean(sequence_output,0)[:, :seq_length]\n",
        "        # print(sequence_output.shape)\n",
        "        # print(pos_output.shape)\n",
        "        sequence_output = torch.cat([sequence_output, pos_output], -1)\n",
        "        # sequence_output = torch.mean(torch.stack([sequence_output,pos_output], dim=0),0)\n",
        "        \n",
        "        # sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output, _ = self.lstm3(sequence_output)\n",
        "        # print(sequence_output.shape)\n",
        "       \n",
        "        sequence_output, _ = self.lstm4(sequence_output)\n",
        "       \n",
        "        \n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        #print(sequence_output.shape)\n",
        "\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNmu9dSaHo4i"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYCwSWVtaiX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504a2af3-bdfa-4f2f-f03b-cf8b06f8e789"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CK8o6hC9TWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e23dec5-f4e3-4420-a819-afd53fdf5f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/882f0cab8dbb456e5b1d6e3b96e864be0cb6c2bc5d20ee88eeda10b3c0317332.a3f80b6502f00efe74a01c6a007f196803229a24224659c81c1da7c4cf5316e8\n",
            "Model config LayoutLMv2Config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"fast_qkv\": true,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": true,\n",
            "  \"has_spatial_attention_bias\": true,\n",
            "  \"has_visual_segment_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"transformers_version\": \"4.22.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3f564f098a94cd2c8fcf76cf65649d4f0482b917cb8552b34e266a0e5d7808a7.f7616351b25aa8250bddb46d3adb2faa9f5f7209ec75f3b4cee8d1dd2c7ca920\n",
            "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2PreTrainedModel: ['layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.weight', 'layoutlmv2.encoder.layer.9.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.bias', 'layoutlmv2.encoder.layer.3.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.2.attention.self.qkv_linear.weight', 'layoutlmv2.encoder.layer.4.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.weight', 'layoutlmv2.encoder.rel_pos_bias.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.running_mean', 'layoutlmv2.embeddings.h_position_embeddings.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.bias', 'layoutlmv2.encoder.layer.5.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.running_mean', 'layoutlmv2.embeddings.token_type_embeddings.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.11.attention.self.v_bias', 'layoutlmv2.encoder.layer.2.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.weight', 'layoutlmv2.visual.backbone.fpn_lateral3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.running_mean', 'layoutlmv2.embeddings.y_position_embeddings.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.weight', 'layoutlmv2.encoder.layer.0.intermediate.dense.weight', 'layoutlmv2.encoder.layer.4.attention.output.LayerNorm.weight', 'layoutlmv2.encoder.layer.11.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.bias', 'layoutlmv2.encoder.layer.6.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.running_var', 'layoutlmv2.encoder.layer.3.attention.output.LayerNorm.weight', 'layoutlmv2.encoder.layer.6.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.6.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.2.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.2.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.running_var', 'layoutlmv2.encoder.layer.7.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.running_var', 'layoutlmv2.encoder.layer.6.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.bias', 'layoutlmv2.encoder.layer.5.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.weight', 'layoutlmv2.encoder.layer.10.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.weight', 'layoutlmv2.encoder.layer.3.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.bias', 'layoutlmv2.encoder.layer.0.output.dense.bias', 'layoutlmv2.encoder.layer.3.attention.self.v_bias', 'layoutlmv2.encoder.layer.8.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.9.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.running_var', 'layoutlmv2.encoder.layer.1.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.11.output.dense.weight', 'layoutlmv2.encoder.layer.6.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.weight', 'layoutlmv2.encoder.layer.0.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.running_mean', 'layoutlmv2.encoder.layer.1.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.weight', 'layoutlmv2.encoder.layer.4.output.dense.weight', 'layoutlmv2.encoder.layer.8.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.weight', 'layoutlmv2.encoder.layer.10.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.weight', 'layoutlmv2.encoder.layer.2.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual_proj.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.running_var', 'layoutlmv2.encoder.layer.9.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.bias', 'layoutlmv2.encoder.layer.5.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.running_var', 'layoutlmv2.encoder.layer.1.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.11.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.7.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.weight', 'layoutlmv2.encoder.layer.9.attention.self.q_bias', 'layoutlmv2.encoder.layer.4.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.weight', 'layoutlmv2.encoder.layer.10.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.running_var', 'layoutlmv2.encoder.layer.3.output.dense.bias', 'layoutlmv2.encoder.layer.1.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.running_var', 'layoutlmv2.encoder.layer.6.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.weight', 'layoutlmv2.embeddings.word_embeddings.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.bias', 'layoutlmv2.encoder.layer.0.attention.self.qkv_linear.weight', 'layoutlmv2.encoder.layer.10.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.1.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.weight', 'layoutlmv2.visual.backbone.fpn_output2.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.8.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.weight', 'layoutlmv2.encoder.layer.9.intermediate.dense.weight', 'layoutlmv2.visual_LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.bias', 'layoutlmv2.encoder.layer.4.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.running_mean', 'layoutlmv2.encoder.layer.2.attention.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.7.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.fpn_lateral2.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.4.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.weight', 'layoutlmv2.visual.backbone.fpn_lateral5.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.bias', 'layoutlmv2.visual.backbone.fpn_lateral3.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.weight', 'layoutlmv2.encoder.layer.2.attention.self.q_bias', 'layoutlmv2.encoder.layer.4.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.running_var', 'layoutlmv2.encoder.layer.11.intermediate.dense.bias', 'layoutlmv2.visual.backbone.fpn_output5.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.9.output.dense.bias', 'layoutlmv2.encoder.layer.8.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.weight', 'layoutlmv2.encoder.layer.2.attention.output.dense.weight', 'layoutlmv2.encoder.layer.10.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.bias', 'layoutlmv2.encoder.layer.4.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.bias', 'layoutlmv2.encoder.layer.8.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.weight', 'layoutlmv2.encoder.layer.9.attention.output.dense.bias', 'layoutlmv2.encoder.layer.1.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.weight', 'layoutlmv2.encoder.layer.0.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.bias', 'layoutlmv2.encoder.layer.11.output.dense.bias', 'layoutlmv2.encoder.layer.5.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.9.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.0.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.10.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.weight', 'layoutlmv2.encoder.layer.6.intermediate.dense.bias', 'layoutlmv2.encoder.layer.7.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.weight', 'layoutlmv2.encoder.layer.11.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.running_var', 'layoutlmv2.encoder.layer.9.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.bias', 'layoutlmv2.encoder.layer.11.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.0.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.running_mean', 'layoutlmv2.embeddings.w_position_embeddings.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.bias', 'layoutlmv2.visual.backbone.fpn_lateral5.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.bias', 'layoutlmv2.encoder.layer.11.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.weight', 'layoutlmv2.encoder.layer.3.attention.output.dense.weight', 'layoutlmv2.encoder.layer.1.attention.self.v_bias', 'layoutlmv2.encoder.rel_pos_y_bias.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.weight', 'layoutlmv2.encoder.layer.5.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.11.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.bias', 'layoutlmv2.encoder.layer.1.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.weight', 'layoutlmv2.encoder.layer.6.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.weight', 'layoutlmv2.encoder.layer.6.attention.output.LayerNorm.weight', 'layoutlmv2.pooler.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.weight', 'layoutlmv2.encoder.layer.6.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.7.intermediate.dense.weight', 'layoutlmv2.encoder.layer.8.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.running_var', 'layoutlmv2.encoder.layer.6.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.fpn_output5.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.bias', 'layoutlmv2.encoder.layer.9.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.7.attention.self.v_bias', 'layoutlmv2.encoder.layer.7.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.bias', 'layoutlmv2.visual_proj.weight', 'layoutlmv2.encoder.layer.7.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.3.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.bias', 'layoutlmv2.encoder.layer.11.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.bias', 'layoutlmv2.encoder.layer.1.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.fpn_output4.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.weight', 'layoutlmv2.visual.pixel_std', 'layoutlmv2.encoder.layer.3.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.fpn_lateral4.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.bias', 'layoutlmv2.encoder.layer.5.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.bias', 'layoutlmv2.encoder.layer.0.attention.output.dense.bias', 'layoutlmv2.embeddings.position_embeddings.weight', 'layoutlmv2.encoder.layer.5.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.running_mean', 'layoutlmv2.encoder.layer.1.attention.output.dense.weight', 'layoutlmv2.encoder.layer.1.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.weight', 'layoutlmv2.encoder.layer.6.attention.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.3.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.weight', 'layoutlmv2.encoder.layer.0.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.3.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.weight', 'layoutlmv2.encoder.layer.10.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.running_mean', 'layoutlmv2.encoder.layer.5.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.weight', 'layoutlmv2.encoder.rel_pos_x_bias.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.weight', 'layoutlmv2.encoder.layer.2.attention.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.9.attention.self.qkv_linear.weight', 'layoutlmv2.encoder.layer.8.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.weight', 'layoutlmv2.encoder.layer.10.intermediate.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.bias', 'layoutlmv2.embeddings.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.7.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.weight', 'layoutlmv2.encoder.layer.8.output.dense.bias', 'layoutlmv2.encoder.layer.7.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.bias', 'layoutlmv2.embeddings.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.running_var', 'layoutlmv2.encoder.layer.7.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.running_var', 'layoutlmv2.visual.pixel_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.running_var', 'layoutlmv2.encoder.layer.2.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.weight', 'layoutlmv2.encoder.layer.2.output.dense.bias', 'layoutlmv2.encoder.layer.8.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.weight', 'layoutlmv2.visual.backbone.fpn_lateral2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.bias', 'layoutlmv2.encoder.layer.8.attention.self.v_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.bias', 'layoutlmv2.pooler.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.weight', 'layoutlmv2.visual.backbone.fpn_output4.weight', 'layoutlmv2.encoder.layer.3.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.weight', 'layoutlmv2.encoder.layer.5.attention.self.qkv_linear.weight', 'layoutlmv2.encoder.layer.3.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.5.output.dense.weight', 'layoutlmv2.encoder.layer.8.attention.output.dense.weight', 'layoutlmv2.encoder.layer.11.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.weight', 'layoutlmv2.encoder.layer.10.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.weight', 'layoutlmv2.encoder.layer.5.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.8.attention.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.bias', 'layoutlmv2.encoder.layer.8.attention.output.LayerNorm.weight', 'layoutlmv2.encoder.layer.1.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.running_var', 'layoutlmv2.visual.backbone.fpn_output3.weight', 'layoutlmv2.encoder.layer.10.attention.self.qkv_linear.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.2.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.running_mean', 'layoutlmv2.encoder.layer.4.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.running_mean', 'layoutlmv2.encoder.layer.10.attention.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.0.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.9.attention.output.LayerNorm.weight', 'layoutlmv2.encoder.layer.10.attention.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.3.attention.output.LayerNorm.bias', 'layoutlmv2.encoder.layer.1.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.weight', 'layoutlmv2.encoder.layer.4.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.0.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.weight', 'layoutlmv2.embeddings.x_position_embeddings.weight', 'layoutlmv2.encoder.layer.9.output.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.running_var', 'layoutlmv2.encoder.layer.7.attention.self.q_bias', 'layoutlmv2.visual.backbone.fpn_output3.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.running_mean', 'layoutlmv2.encoder.layer.2.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.running_var', 'layoutlmv2.encoder.layer.5.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.weight', 'layoutlmv2.encoder.layer.0.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.4.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.weight', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.running_var', 'layoutlmv2.encoder.layer.6.output.dense.bias', 'layoutlmv2.encoder.layer.4.output.dense.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.weight', 'layoutlmv2.visual.backbone.fpn_lateral4.bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.weight', 'layoutlmv2.encoder.layer.10.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.running_var', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.bias', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.weight', 'layoutlmv2.encoder.layer.0.output.LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.encoder.layer.4.attention.self.q_bias', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.running_mean', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.bias', 'layoutlmv2.encoder.layer.5.intermediate.dense.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.running_mean', 'layoutlmv2.visual.backbone.fpn_output2.weight', 'layoutlmv2.encoder.layer.7.attention.output.LayerNorm.bias', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.weight', 'layoutlmv2.visual_LayerNorm.weight', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.running_var', 'layoutlmv2.encoder.layer.11.intermediate.dense.weight']\n",
            "- This IS expected if you are initializing LayoutLMv2PreTrainedModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LayoutLMv2PreTrainedModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of LayoutLMv2PreTrainedModel were initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMv2PreTrainedModel for predictions without further training.\n",
            "loading configuration file https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/882f0cab8dbb456e5b1d6e3b96e864be0cb6c2bc5d20ee88eeda10b3c0317332.a3f80b6502f00efe74a01c6a007f196803229a24224659c81c1da7c4cf5316e8\n",
            "Model config LayoutLMv2Config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"convert_sync_batchnorm\": true,\n",
            "  \"coordinate_size\": 128,\n",
            "  \"detectron2_config_args\": {\n",
            "    \"MODEL.ANCHOR_GENERATOR.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.BACKBONE.NAME\": \"build_resnet_fpn_backbone\",\n",
            "    \"MODEL.FPN.IN_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.MASK_ON\": true,\n",
            "    \"MODEL.PIXEL_STD\": [\n",
            "      57.375,\n",
            "      57.12,\n",
            "      58.395\n",
            "    ],\n",
            "    \"MODEL.POST_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RESNETS.ASPECT_RATIOS\": [\n",
            "      [\n",
            "        0.5,\n",
            "        1.0,\n",
            "        2.0\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.DEPTH\": 101,\n",
            "    \"MODEL.RESNETS.NUM_GROUPS\": 32,\n",
            "    \"MODEL.RESNETS.OUT_FEATURES\": [\n",
            "      \"res2\",\n",
            "      \"res3\",\n",
            "      \"res4\",\n",
            "      \"res5\"\n",
            "    ],\n",
            "    \"MODEL.RESNETS.SIZES\": [\n",
            "      [\n",
            "        32\n",
            "      ],\n",
            "      [\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        128\n",
            "      ],\n",
            "      [\n",
            "        256\n",
            "      ],\n",
            "      [\n",
            "        512\n",
            "      ]\n",
            "    ],\n",
            "    \"MODEL.RESNETS.STRIDE_IN_1X1\": false,\n",
            "    \"MODEL.RESNETS.WIDTH_PER_GROUP\": 8,\n",
            "    \"MODEL.ROI_BOX_HEAD.NAME\": \"FastRCNNConvFCHead\",\n",
            "    \"MODEL.ROI_BOX_HEAD.NUM_FC\": 2,\n",
            "    \"MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\": 14,\n",
            "    \"MODEL.ROI_HEADS.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\"\n",
            "    ],\n",
            "    \"MODEL.ROI_HEADS.NAME\": \"StandardROIHeads\",\n",
            "    \"MODEL.ROI_HEADS.NUM_CLASSES\": 5,\n",
            "    \"MODEL.ROI_MASK_HEAD.NAME\": \"MaskRCNNConvUpsampleHead\",\n",
            "    \"MODEL.ROI_MASK_HEAD.NUM_CONV\": 4,\n",
            "    \"MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION\": 7,\n",
            "    \"MODEL.RPN.IN_FEATURES\": [\n",
            "      \"p2\",\n",
            "      \"p3\",\n",
            "      \"p4\",\n",
            "      \"p5\",\n",
            "      \"p6\"\n",
            "    ],\n",
            "    \"MODEL.RPN.POST_NMS_TOPK_TRAIN\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TEST\": 1000,\n",
            "    \"MODEL.RPN.PRE_NMS_TOPK_TRAIN\": 2000\n",
            "  },\n",
            "  \"fast_qkv\": true,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"has_relative_attention_bias\": true,\n",
            "  \"has_spatial_attention_bias\": true,\n",
            "  \"has_visual_segment_embedding\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_feature_pool_shape\": [\n",
            "    7,\n",
            "    7,\n",
            "    256\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_2d_position_embeddings\": 1024,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_rel_2d_pos\": 256,\n",
            "  \"max_rel_pos\": 128,\n",
            "  \"model_type\": \"layoutlmv2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rel_2d_pos_bins\": 64,\n",
            "  \"rel_pos_bins\": 32,\n",
            "  \"shape_size\": 128,\n",
            "  \"transformers_version\": \"4.22.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3f564f098a94cd2c8fcf76cf65649d4f0482b917cb8552b34e266a0e5d7808a7.f7616351b25aa8250bddb46d3adb2faa9f5f7209ec75f3b4cee8d1dd2c7ca920\n",
            "using `AvgPool2d` instead of `AdaptiveAvgPool2d`\n",
            "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2Model: ['layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked']\n",
            "- This IS expected if you are initializing LayoutLMv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LayoutLMv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of LayoutLMv2Model were initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LayoutLMv2Model for predictions without further training.\n",
            "loading configuration file https://huggingface.co/google/byt5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fb65469caa8fbee829a31eb78e8a08e6a325163dfa747156f69015eccaf393f9.49e4eab2f5208ee37f1fe4b4af5f15482fa80bff71a1f5e694d804a96212a656\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"/home/patrick/t5/byt5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3584,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1472,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 4,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"ByT5Tokenizer\",\n",
            "  \"transformers_version\": \"4.22.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 384\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/google/byt5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f96ed66b9273913f63eb8d2ef2e710977e5f64daf2a57d33d062f442ba957d03.1ee19dbbc64d3111533245dc19557e3b4925ffd9477a66c1d834215871af8ddc\n",
            "Some weights of the model checkpoint at google/byt5-small were not used when initializing T5EncoderModel: ['decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.final_layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'lm_head.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight']\n",
            "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of T5EncoderModel were initialized from the model checkpoint at google/byt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5EncoderModel for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from datasets import load_metric\n",
        "import json\n",
        "from transformers import get_scheduler\n",
        "\n",
        "model = LayoutLMv2PreTrainedModel.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "model.config.num_labels = len(id2label)\n",
        "models = LayoutLMv2ForTokenClassification2(model.config)\n",
        "\n",
        "for p in models.model_char.parameters():\n",
        "  p.requires_grad = False\n",
        "for p in models.layoutlmv2.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "# Set id2label and label2id \n",
        "\n",
        "# Metrics\n",
        "metric = load_metric(\"seqeval\")\n",
        "return_entity_level_metrics = True\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return json.dump({\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "class FunsdTrainer(Trainer):\n",
        "    def get_train_dataloader(self):\n",
        "      return train_dataloader\n",
        "\n",
        "    def get_test_dataloader(self, test_dataset):\n",
        "      return test_dataloader\n",
        "\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule, AdamW\n",
        "#optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "\n",
        "optimizer = AdamW(params=models.parameters(), lr=0.008)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=2000,\n",
        ")\n",
        "args = TrainingArguments(\n",
        "    \n",
        "    output_dir=\"layoutlmv2-finetuned-funsd-v2\", # name of directory to store the checkpoints\n",
        "    max_steps=2000, # we train for a maximum of 1,000 batches\n",
        "    warmup_ratio=0.4, # we warmup a bit\n",
        "    \n",
        ")\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = FunsdTrainer(\n",
        "    model=models,\n",
        "    args=args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, lr_scheduler)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6CpRRIRHjLy"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in models.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTClyLleHkz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c973641c-7fdd-4ddd-a03f-26ce2ff426f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1634523"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "total_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "EifKkBYB9YrF",
        "outputId": "acde59c5-9f69-4861-888f-ad8389c40f64"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 626\n",
            "  Num Epochs = 13\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='673' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 673/2000 15:46 < 31:11, 0.71 it/s, Epoch 4.28/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.073400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-500\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-500/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-500/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 47:02, Epoch 12/13]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.073400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.019200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.002700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-1000\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1000/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-1500\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1500/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to layoutlmv2-finetuned-funsd-v2/checkpoint-2000\n",
            "Configuration saved in layoutlmv2-finetuned-funsd-v2/checkpoint-2000/config.json\n",
            "Model weights saved in layoutlmv2-finetuned-funsd-v2/checkpoint-2000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=0.025929803729057312, metrics={'train_runtime': 2824.7953, 'train_samples_per_second': 5.664, 'train_steps_per_second': 0.708, 'total_flos': 9664787726524416.0, 'train_loss': 0.025929803729057312, 'epoch': 12.74})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "trainer.train()\n",
        "# predictions, labels, metrics = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9Sj7wxZz5-L"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3127xG8qR6-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from seqeval.metrics import (\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it8lQd2QGgcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "449122dd-b84c-41b7-c85b-aa93fa23b07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/174 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|          | 1/174 [00:00<01:58,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|          | 2/174 [00:01<01:57,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   2%|         | 3/174 [00:02<01:56,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   2%|         | 4/174 [00:02<01:55,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   3%|         | 5/174 [00:03<01:54,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   3%|         | 6/174 [00:04<01:53,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   4%|         | 7/174 [00:04<01:53,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   5%|         | 8/174 [00:05<01:52,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   5%|         | 9/174 [00:06<01:52,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   6%|         | 10/174 [00:06<01:51,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   6%|         | 11/174 [00:07<01:50,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   7%|         | 12/174 [00:08<01:49,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   7%|         | 13/174 [00:08<01:48,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   8%|         | 14/174 [00:09<01:47,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   9%|         | 15/174 [00:10<01:47,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   9%|         | 16/174 [00:10<01:47,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  10%|         | 17/174 [00:11<01:46,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  10%|         | 18/174 [00:12<01:45,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  11%|         | 19/174 [00:12<01:45,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  11%|        | 20/174 [00:13<01:44,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  12%|        | 21/174 [00:14<01:43,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  13%|        | 22/174 [00:14<01:43,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  13%|        | 23/174 [00:15<01:42,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  14%|        | 24/174 [00:16<01:41,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  14%|        | 25/174 [00:16<01:42,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  15%|        | 26/174 [00:17<01:41,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  16%|        | 27/174 [00:18<01:40,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  16%|        | 28/174 [00:19<01:40,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  17%|        | 29/174 [00:19<01:39,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  17%|        | 30/174 [00:20<01:38,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  18%|        | 31/174 [00:21<01:37,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  18%|        | 32/174 [00:21<01:36,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  19%|        | 33/174 [00:22<01:35,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|        | 34/174 [00:23<01:34,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  20%|        | 35/174 [00:23<01:34,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  21%|        | 36/174 [00:24<01:34,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  21%|       | 37/174 [00:25<01:34,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  22%|       | 38/174 [00:25<01:33,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  22%|       | 39/174 [00:26<01:32,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  23%|       | 40/174 [00:27<01:31,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  24%|       | 41/174 [00:27<01:30,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  24%|       | 42/174 [00:28<01:29,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  25%|       | 43/174 [00:29<01:29,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  25%|       | 44/174 [00:29<01:28,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  26%|       | 45/174 [00:30<01:27,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  26%|       | 46/174 [00:31<01:27,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  27%|       | 47/174 [00:32<01:26,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  28%|       | 48/174 [00:32<01:25,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  28%|       | 49/174 [00:33<01:25,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  29%|       | 50/174 [00:34<01:24,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  29%|       | 51/174 [00:34<01:23,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  30%|       | 52/174 [00:35<01:22,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  30%|       | 53/174 [00:36<01:21,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  31%|       | 54/174 [00:36<01:21,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  32%|      | 55/174 [00:37<01:20,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  32%|      | 56/174 [00:38<01:19,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  33%|      | 57/174 [00:38<01:19,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  33%|      | 58/174 [00:39<01:18,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  34%|      | 59/174 [00:40<01:17,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  34%|      | 60/174 [00:40<01:17,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  35%|      | 61/174 [00:41<01:16,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  36%|      | 62/174 [00:42<01:16,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  36%|      | 63/174 [00:42<01:15,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  37%|      | 64/174 [00:43<01:14,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  37%|      | 65/174 [00:44<01:14,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  38%|      | 66/174 [00:44<01:13,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  39%|      | 67/174 [00:45<01:12,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  39%|      | 68/174 [00:46<01:12,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  40%|      | 69/174 [00:46<01:11,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  40%|      | 70/174 [00:47<01:10,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  41%|      | 71/174 [00:48<01:10,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  41%|     | 72/174 [00:48<01:09,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  42%|     | 73/174 [00:49<01:09,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  43%|     | 74/174 [00:50<01:08,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  43%|     | 75/174 [00:51<01:07,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  44%|     | 76/174 [00:51<01:06,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  44%|     | 77/174 [00:52<01:06,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  45%|     | 78/174 [00:53<01:05,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  45%|     | 79/174 [00:53<01:04,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  46%|     | 80/174 [00:54<01:03,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  47%|     | 81/174 [00:55<01:03,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  47%|     | 82/174 [00:55<01:02,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  48%|     | 83/174 [00:56<01:02,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  48%|     | 84/174 [00:57<01:01,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  49%|     | 85/174 [00:57<01:00,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  49%|     | 86/174 [00:58<00:59,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  50%|     | 87/174 [00:59<00:59,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  51%|     | 88/174 [00:59<00:58,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  51%|     | 89/174 [01:00<00:58,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  52%|    | 90/174 [01:01<00:57,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  52%|    | 91/174 [01:01<00:56,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  53%|    | 92/174 [01:02<00:55,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  53%|    | 93/174 [01:03<00:54,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  54%|    | 94/174 [01:03<00:54,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  55%|    | 95/174 [01:04<00:53,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  55%|    | 96/174 [01:05<00:52,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  56%|    | 97/174 [01:05<00:52,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  56%|    | 98/174 [01:06<00:51,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  57%|    | 99/174 [01:07<00:50,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  57%|    | 100/174 [01:08<00:50,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  58%|    | 101/174 [01:08<00:49,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  59%|    | 102/174 [01:09<00:48,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  59%|    | 103/174 [01:10<00:47,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  60%|    | 104/174 [01:10<00:47,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  60%|    | 105/174 [01:11<00:46,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  61%|    | 106/174 [01:12<00:45,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  61%|   | 107/174 [01:12<00:45,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  62%|   | 108/174 [01:13<00:44,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  63%|   | 109/174 [01:14<00:43,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  63%|   | 110/174 [01:14<00:43,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  64%|   | 111/174 [01:15<00:42,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  64%|   | 112/174 [01:16<00:41,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  65%|   | 113/174 [01:16<00:41,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  66%|   | 114/174 [01:17<00:40,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  66%|   | 115/174 [01:18<00:39,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  67%|   | 116/174 [01:18<00:39,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  67%|   | 117/174 [01:19<00:38,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  68%|   | 118/174 [01:20<00:37,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  68%|   | 119/174 [01:20<00:37,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  69%|   | 120/174 [01:21<00:36,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  70%|   | 121/174 [01:22<00:36,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  70%|   | 122/174 [01:22<00:35,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  71%|   | 123/174 [01:23<00:34,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  71%|  | 124/174 [01:24<00:34,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  72%|  | 125/174 [01:24<00:33,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  72%|  | 126/174 [01:25<00:32,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  73%|  | 127/174 [01:26<00:32,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  74%|  | 128/174 [01:27<00:31,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  74%|  | 129/174 [01:27<00:30,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  75%|  | 130/174 [01:28<00:30,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  75%|  | 131/174 [01:29<00:29,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  76%|  | 132/174 [01:29<00:28,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  76%|  | 133/174 [01:30<00:27,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  77%|  | 134/174 [01:31<00:27,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  78%|  | 135/174 [01:31<00:26,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  78%|  | 136/174 [01:32<00:25,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  79%|  | 137/174 [01:33<00:25,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  79%|  | 138/174 [01:33<00:24,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  80%|  | 139/174 [01:34<00:23,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  80%|  | 140/174 [01:35<00:23,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  81%|  | 141/174 [01:35<00:22,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  82%| | 142/174 [01:36<00:21,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  82%| | 143/174 [01:37<00:21,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  83%| | 144/174 [01:37<00:20,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  83%| | 145/174 [01:38<00:19,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  84%| | 146/174 [01:39<00:19,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  84%| | 147/174 [01:39<00:18,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  85%| | 148/174 [01:40<00:17,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  86%| | 149/174 [01:41<00:17,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  86%| | 150/174 [01:42<00:16,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  87%| | 151/174 [01:42<00:15,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  87%| | 152/174 [01:43<00:15,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  88%| | 153/174 [01:44<00:14,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  89%| | 154/174 [01:44<00:13,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  89%| | 155/174 [01:45<00:13,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  90%| | 156/174 [01:46<00:12,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  90%| | 157/174 [01:46<00:11,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  91%| | 158/174 [01:47<00:10,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  91%|| 159/174 [01:48<00:10,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  92%|| 160/174 [01:48<00:09,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  93%|| 161/174 [01:49<00:08,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  93%|| 162/174 [01:50<00:08,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  94%|| 163/174 [01:50<00:07,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  94%|| 164/174 [01:51<00:06,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  95%|| 165/174 [01:52<00:06,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  95%|| 166/174 [01:52<00:05,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  96%|| 167/174 [01:53<00:04,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  97%|| 168/174 [01:54<00:04,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  97%|| 169/174 [01:54<00:03,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  98%|| 170/174 [01:55<00:02,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  98%|| 171/174 [01:56<00:02,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  99%|| 172/174 [01:57<00:01,  1.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  99%|| 173/174 [01:57<00:00,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['image', 'input_ids', 'embedding_char_ids', 'attention_mask', 'token_type_ids', 'bbox', 'labels', 'pos_tags', 'dep_tags'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 174/174 [01:58<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.03087681388053337, 'precision': 0.9353448275862069, 'recall': 0.9380403458213257, 'f1': 0.9366906474820144}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "eval_loss = 0.0\n",
        "nb_eval_steps = 0\n",
        "preds = None\n",
        "out_label_ids = None\n",
        "\n",
        "# put model in evaluation mode\n",
        "models.eval()\n",
        "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "    with torch.no_grad():\n",
        "        print(batch.keys())\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        bbox = batch[\"bbox\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        image = batch['image'].to(device)\n",
        "        pos_tags = batch['pos_tags'].to(device)\n",
        "        dep_tags = batch['pos_tags'].to(device)\n",
        "\n",
        "        input_ids_char = batch[\"embedding_char_ids\"].to(device)\n",
        "\n",
        "        # forward pass\n",
        "        outputs = models( embedding_char_ids =input_ids_char, pos_tags = pos_tags,dep_tags = dep_tags,image = image , input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,labels=labels)\n",
        "        # get the loss and logits\n",
        "        tmp_eval_loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        eval_loss += tmp_eval_loss.item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # compute the predictions\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = labels.detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(\n",
        "                out_label_ids, labels.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "\n",
        "# compute average evaluation loss\n",
        "eval_loss = eval_loss / nb_eval_steps\n",
        "preds = np.argmax(preds, axis=2)\n",
        "\n",
        "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "\n",
        "for i in range(out_label_ids.shape[0]):\n",
        "    for j in range(out_label_ids.shape[1]):\n",
        "        if out_label_ids[i, j] != -100:\n",
        "            out_label_list[i].append(id2label[out_label_ids[i][j]])\n",
        "            preds_list[i].append(id2label[preds[i][j]])\n",
        "\n",
        "results = {\n",
        "    \"loss\": eval_loss,\n",
        "    \"precision\": precision_score(out_label_list, preds_list),\n",
        "    \"recall\": recall_score(out_label_list, preds_list),\n",
        "    \"f1\": f1_score(out_label_list, preds_list),\n",
        "}\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyWzOlv8mb1U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "grupostagccharembeddingdeptag_layoutlm2.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "246aec5c31324290a68ef4471b7d75c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_276eee742f474c9d94a341d38a6ebaf8",
              "IPY_MODEL_4561803ed1294a08a68d1fb9223d71b6",
              "IPY_MODEL_a4fd1fa070094f089d5c58a38bdd16d8"
            ],
            "layout": "IPY_MODEL_aa02c64aa2fe43329861349fd2e8a716"
          }
        },
        "276eee742f474c9d94a341d38a6ebaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2419b791e9044f17a8b4674ada744794",
            "placeholder": "",
            "style": "IPY_MODEL_bb08562dcb3b4ba79edffdf5d0312e5e",
            "value": "Downloading data files: 100%"
          }
        },
        "4561803ed1294a08a68d1fb9223d71b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d61a7c618640c99dfcc6e4c9ac697d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ffb128171c4ba69cbcfce042021d36",
            "value": 1
          }
        },
        "a4fd1fa070094f089d5c58a38bdd16d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_828f1097a4194979926635b9c0d499ac",
            "placeholder": "",
            "style": "IPY_MODEL_de0d538f088344d482619681231208fe",
            "value": " 1/1 [00:00&lt;00:00, 34.90it/s]"
          }
        },
        "aa02c64aa2fe43329861349fd2e8a716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2419b791e9044f17a8b4674ada744794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb08562dcb3b4ba79edffdf5d0312e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1d61a7c618640c99dfcc6e4c9ac697d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ffb128171c4ba69cbcfce042021d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "828f1097a4194979926635b9c0d499ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de0d538f088344d482619681231208fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64297f9091674e51a1fa974076162394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be1770715bbb4aaea456374fd98dc49a",
              "IPY_MODEL_99f038d419334c2784c6790c5b26d9a6",
              "IPY_MODEL_0a92346fa836489690824fb5967105b3"
            ],
            "layout": "IPY_MODEL_422f86fe0b1f47e7a3d3e9665a78f1ca"
          }
        },
        "be1770715bbb4aaea456374fd98dc49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f8b1dbd45f47f3b958c6f6599d6547",
            "placeholder": "",
            "style": "IPY_MODEL_0d96c3d182f1490f9b2b68b90e2e5fba",
            "value": "Extracting data files: 100%"
          }
        },
        "99f038d419334c2784c6790c5b26d9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56faee18ba0747c6a50099238e64575b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb796ed667134387bb75ba53a303364c",
            "value": 1
          }
        },
        "0a92346fa836489690824fb5967105b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea279761b3904828b1193ac8bac88cb7",
            "placeholder": "",
            "style": "IPY_MODEL_010fc26ad69543628aaa6124f857f2cd",
            "value": " 1/1 [00:00&lt;00:00, 33.27it/s]"
          }
        },
        "422f86fe0b1f47e7a3d3e9665a78f1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f8b1dbd45f47f3b958c6f6599d6547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d96c3d182f1490f9b2b68b90e2e5fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56faee18ba0747c6a50099238e64575b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb796ed667134387bb75ba53a303364c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea279761b3904828b1193ac8bac88cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "010fc26ad69543628aaa6124f857f2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cddd4190bdf64465a620e828f3a33a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4bfcb3ddbc44daa85afc9d2a75b6822",
              "IPY_MODEL_14da34fff6c24d40a253abe9723d6deb",
              "IPY_MODEL_d9055168444c409b930bac9a6138caac"
            ],
            "layout": "IPY_MODEL_f8e85cae3fbd44c6af1487b3c2edd36c"
          }
        },
        "b4bfcb3ddbc44daa85afc9d2a75b6822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4669cfc1bb7491fb875c09cb890670b",
            "placeholder": "",
            "style": "IPY_MODEL_01c7951c37294ae5873ca753b1de88f9",
            "value": "Generating train split: "
          }
        },
        "14da34fff6c24d40a253abe9723d6deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4fbe9d0466c4004ad65c84f60ee5d5e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa0399073b547ed8018ce4850bc59ab",
            "value": 1
          }
        },
        "d9055168444c409b930bac9a6138caac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9aceb33b3a244f38b8b31bac9efd3fc1",
            "placeholder": "",
            "style": "IPY_MODEL_1ac110272a7d4520a4f2f7101d0963c6",
            "value": " 603/0 [00:01&lt;00:00, 475.04 examples/s]"
          }
        },
        "f8e85cae3fbd44c6af1487b3c2edd36c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4669cfc1bb7491fb875c09cb890670b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c7951c37294ae5873ca753b1de88f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4fbe9d0466c4004ad65c84f60ee5d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6fa0399073b547ed8018ce4850bc59ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9aceb33b3a244f38b8b31bac9efd3fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac110272a7d4520a4f2f7101d0963c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb892fa67f374b1285d808d93c2903d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97de47bad8c443649bae9cebbbfc2666",
              "IPY_MODEL_89f59220bb5842e39b8d561378928d3a",
              "IPY_MODEL_7d33d6638b9644a9b89cfeac03280146"
            ],
            "layout": "IPY_MODEL_5ef2631b6615438f8f6c2e609eca43f7"
          }
        },
        "97de47bad8c443649bae9cebbbfc2666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb406b03d0dc440ca99a205cfa33df62",
            "placeholder": "",
            "style": "IPY_MODEL_24388f62ab9a4fc5bd7a23317802cfd8",
            "value": "Generating test split: "
          }
        },
        "89f59220bb5842e39b8d561378928d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d326a579c668430f987d0d341776b752",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9789dc9a7dcd4c5ba36abbe2579dee9b",
            "value": 1
          }
        },
        "7d33d6638b9644a9b89cfeac03280146": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa26524f49f04150b688e38e597cd0bd",
            "placeholder": "",
            "style": "IPY_MODEL_4eddc6e776684052941d07d8b7e36b01",
            "value": " 309/0 [00:01&lt;00:00, 310.91 examples/s]"
          }
        },
        "5ef2631b6615438f8f6c2e609eca43f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb406b03d0dc440ca99a205cfa33df62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24388f62ab9a4fc5bd7a23317802cfd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d326a579c668430f987d0d341776b752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9789dc9a7dcd4c5ba36abbe2579dee9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa26524f49f04150b688e38e597cd0bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eddc6e776684052941d07d8b7e36b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b792acc3114c86bfbdd4af0be07232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eaf1ab09c4b48fba5734667a48b5073",
              "IPY_MODEL_7ccc4a870dd0424a89f8124a7673372d",
              "IPY_MODEL_2f3e05f28dde40589ccbf083139ff001"
            ],
            "layout": "IPY_MODEL_cbb44d65cddb4a0c89eb5a3db6367632"
          }
        },
        "2eaf1ab09c4b48fba5734667a48b5073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22b4caeafc65455fb5fc5f9e5750e2bf",
            "placeholder": "",
            "style": "IPY_MODEL_7d4dd5a92bd34149818fc12c7acf38ca",
            "value": "100%"
          }
        },
        "7ccc4a870dd0424a89f8124a7673372d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5814ff9c3e68418d9e5a71713136f10e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6078fc2d150e423293d2d226fc4cc92b",
            "value": 2
          }
        },
        "2f3e05f28dde40589ccbf083139ff001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9b79b07c3654f6e95e7442d923641fa",
            "placeholder": "",
            "style": "IPY_MODEL_291bf0c8c5fd461f9715a6cad21cc4a3",
            "value": " 2/2 [00:00&lt;00:00, 54.71it/s]"
          }
        },
        "cbb44d65cddb4a0c89eb5a3db6367632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22b4caeafc65455fb5fc5f9e5750e2bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d4dd5a92bd34149818fc12c7acf38ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5814ff9c3e68418d9e5a71713136f10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6078fc2d150e423293d2d226fc4cc92b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9b79b07c3654f6e95e7442d923641fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291bf0c8c5fd461f9715a6cad21cc4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "838ee8a405034ce89620b601eee7c7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75f5be29e26b4d8da23ae63327f3966f",
              "IPY_MODEL_98b544367f2d45e48fb8d5b772348c74",
              "IPY_MODEL_174327660f484a6688301e98f9537f48"
            ],
            "layout": "IPY_MODEL_be6f0ddee53b4c928dec0ceacab867be"
          }
        },
        "75f5be29e26b4d8da23ae63327f3966f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bafff4fce3493081c64ae4b8782591",
            "placeholder": "",
            "style": "IPY_MODEL_0cbc8581e58542e9b27fbec15075fe15",
            "value": "100%"
          }
        },
        "98b544367f2d45e48fb8d5b772348c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_491f437e2baf4e25afd89caff5dfab69",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a6a1fea07884d1fa913249791ac06a2",
            "value": 1
          }
        },
        "174327660f484a6688301e98f9537f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb243e14b3114e0e963bc437042dc29e",
            "placeholder": "",
            "style": "IPY_MODEL_f980c393edad4f26a4b696971b7dc627",
            "value": " 1/1 [01:49&lt;00:00, 109.20s/ba]"
          }
        },
        "be6f0ddee53b4c928dec0ceacab867be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02bafff4fce3493081c64ae4b8782591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cbc8581e58542e9b27fbec15075fe15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "491f437e2baf4e25afd89caff5dfab69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6a1fea07884d1fa913249791ac06a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb243e14b3114e0e963bc437042dc29e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f980c393edad4f26a4b696971b7dc627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7634c69e934749149650f0acb5a39f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47fef0a821fc409196bfb8be3756056a",
              "IPY_MODEL_7932d2cf8cc54b3fb214e28dba750df7",
              "IPY_MODEL_04e0f2d0a1c9402399c593e5834b3a21"
            ],
            "layout": "IPY_MODEL_153d9521b43a46e88bfb1492bc0a301b"
          }
        },
        "47fef0a821fc409196bfb8be3756056a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56389c21ec14cb3a981ff85f0f11704",
            "placeholder": "",
            "style": "IPY_MODEL_85ed8edba42241e4967770c8120b69c3",
            "value": "100%"
          }
        },
        "7932d2cf8cc54b3fb214e28dba750df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e8e73cd9f1c44adb3ea1cc88624a058",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d4a049eed6c401ba081983b4c7c93f9",
            "value": 1
          }
        },
        "04e0f2d0a1c9402399c593e5834b3a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed99d3853981478e9262c9d92cfa65a4",
            "placeholder": "",
            "style": "IPY_MODEL_0ee2b543b20b4e81a2f5ac591a2537ff",
            "value": " 1/1 [01:00&lt;00:00, 60.82s/ba]"
          }
        },
        "153d9521b43a46e88bfb1492bc0a301b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56389c21ec14cb3a981ff85f0f11704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85ed8edba42241e4967770c8120b69c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8e73cd9f1c44adb3ea1cc88624a058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4a049eed6c401ba081983b4c7c93f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed99d3853981478e9262c9d92cfa65a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ee2b543b20b4e81a2f5ac591a2537ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}